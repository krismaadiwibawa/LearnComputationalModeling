---
title: "LearnComputationalModeling"
author: "Krisma Adiwibawa"
format:
  pdf:
    code-overflow: wrap
    include-in-header: _preamble.tex
editor: visual
---

## Chapter 2: From Words to Models

### 2.2.2 The Random Walk Model

Listing 2.1 A simple random-walk model

```{r listing_2.1}

#random-walk model
nreps <- 10000 #number of random-walks (decisions)
nsamples <- 2000 #number of times that evidence is being sampled for each decision

drift <- 0.0 #noninformative stimulus
             #drift rate: amount of evidence available during sampling
sdrw <- 0.3  #noise in the evidence as standard deviation
criterion <- 3 #response criterion (the distance of the 2 boundaries from origin)

latencies <- rep(0, nreps) #vector containing zeros
responses <- rep(0, nreps) #vector containing zeros
evidence <- matrix(0, nreps, nsamples+1) #a matric containing zeros
for (i in c(1:nreps)) {
  evidence[i,] <- cumsum(c(0, rnorm(nsamples, drift, sdrw))) #evidence accumulation
  p <- which(abs(evidence[i,])>criterion)[1] #the step at which decision is made (the column)
  responses[i] <- sign(evidence[i,p]) #return +1 or -1
  latencies[i] <- p
}

#View(evidence)
```

Listing 2.2 Plot up to 5 random-walk paths

```{r listing_2.2, fig.width=10, fig.height=6}

#plot up to 5 random-walk paths
tbpn <- min(nreps, 5) #to-be-plotted-number
plot(1:max(latencies[1:tbpn])+10, # create x-axis as needed (what the longest latency?
     type="n", #create empty plot
     las=1, #make the y-axis labels horizontal
     ylim=c(-criterion-.5, criterion+.5), #set y-axis limit, give extra room above and below criterion
     ylab="Evidence", xlab="Decision time")
for (i in c(1:tbpn)) {
       lines(evidence[i, 1:(latencies[i]-1)]) #this is the values in row i, which is the evidence sampling. -1 means plot the line just before the decision
}
abline(h=c(criterion, -criterion), lty="dashed") #create a dashed line for criterion (upper & lower decision treshold)
```

Listing 2.3 Plot distribution of response latencies from random-walk

```{r listing_2.3, fig.width=10, fig.height=8}

#plot histogram latencies
par(mfrow=c(2,1)) #split the plotting area into 2 rows and 1 column
toprt <- latencies[responses>0] #latencies that corresponds to +1
topprop <- length(toprt)/nreps #proportion of top responses relative to all trials
hist(toprt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Top responses (", as.numeric(topprop),
     ") m=", as.character(signif(mean(toprt),4)),
     sep=""), las=1)
botrt <- latencies[responses<0] #latencies that corresponds to -1
botprop <- length(botrt)/nreps
hist(botrt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Bottom responses (", as.numeric(botprop),
                ") m=", as.character(signif(mean(botrt), 4))
                )
     )
```

Listing 2.4 A random-walk model with trial-to-trial variability

```{r listing_2.4}

#random walk model with unequal latencies between responses classes
nreps <- 1000
nsamples <- 2000

drift <- 0.03  # 0 = noninformative stimulus; >0 = informative
sdrw <- 0.3
criterion <- 3 
t2tsd  <- c(0.0,0.025)

latencies <- rep(0,nreps)
responses <- rep(0,nreps)
evidence <- matrix(0, nreps, nsamples+1) 
for (i in c(1:nreps)) { 
  sp <- rnorm(1,0,t2tsd[1]) 
  dr <- rnorm(1,drift,t2tsd[2]) 
  evidence[i,] <- cumsum(c(sp,rnorm(nsamples,dr,sdrw))) 
  p <-  which(abs(evidence[i,])>criterion)[1]
  responses[i] <- sign(evidence[i,p])
  latencies[i]  <- p
}
```

Listing 2.5

```{r listing_2.5}

#plot up to 5 random walk paths
tbpn <- min(nreps,5)
plot(1:max(latencies[1:tbpn])+10,type="n",las=1,
     ylim=c(-criterion-.5,criterion+.5),
     ylab="Evidence",xlab="Decision time")
for (i in c(1:tbpn)) {
  lines(evidence[i,1:(latencies[i]-1)])   
}
abline(h=c(criterion,-criterion),lty="dashed")  
```

Listing 2.6

```{r listing_2.6}

#plot histograms of latencies
par(mfrow=c(2,1))
toprt <- latencies[responses>0]
topprop <- length(toprt)/nreps
hist(toprt,col="gray",
     xlab="Decision time", xlim=c(0,max(latencies)),
     main=paste("Top responses (",as.numeric(topprop),
          ") m=",as.character(signif(mean(toprt),4)),
          sep=""),las=1)
botrt <- latencies[responses<0]
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Bottom responses (",as.numeric(botprop),
          ") m=",as.character(signif(mean(botrt),4)),
          sep=""),las=1)
```

## Chapter 4: Maximum Likelihood Parameter Estimation

Listing 4.1

```{r listing_4.1}

rswald <- function(t, a, m, Ter) {
  ans <- a/sqrt(2*pi*(t-Ter)^3)*exp(-(a-m*(t-Ter))^2/(2*(t-Ter)))
}

# The shifted Wald probability density function
```

Listing 4.2

```{r listing_4.2}

# This code implements the core calculations of the General Context Model (GCM), a highly influential model in cognitive psychology used to predict how people categorize a new stimulus based on their memory of past examples (exemplars).

source("GCMpred.R")
# The function that implements the mathematical core of the GCM (stored in GCMpred.R file), calculating the predicted probability of a specific category response (e.g., Category A).

N <- 2*80 # there were 2 responses nper face from 80 people
N_A <- round(N*.968) #N_B is implicitly N - N_A. This is the actual experimental data the model needs to explain (In the data collected there are 155 "A"-responses out of 160).

c <- 4 # best-fitting parameter from Nosofsky (1991) 
w <- c(0.19, 0.12, 0.25, 0.45) # best-fitting parameter from Nosofsky (1991)

stim <- as.matrix(read.table("faceStim.csv", sep=",")) # The stimuli used

exemplars <- list(a=stim[1:5,], b=stim[6:10,]) # The exemplar is taken from the first 5 rows of the faceStim.csv (Exemplar for category A) and the next 5 rows (Exemplar for category B)

preds <- GCMpred(stim[1,], exemplars, c, w) # This runs the GCM. It takes only the first stimulus (stim[1,]) and asks: Based on the memory set and the chosen parameters, what is the probability of categorizing this specific probe as A or B?

likelihood <- dbinom(N_A, size=N, prob=preds[1])
# This is the crucial step linking the model prediction to the data using the Binomial Likelihood Function; dbinom() calculates the probability of observing exactly N(A) 'A' responses out of N total responses, given the model's predicted probability (preds[1]).

# The term likelihood is used because we want to calculate what is the likelihood of the parameter values we used above, GIVEN the N_A we observe in the collected data. In the parameter estimation of maximum likelihood method, we are looking for the parameter values that return the highest likelihood, given N_A (the data we collected)

```

Listing 4.4

```{r listing_4.5}

source("GCMprednoisy.R")
library(dfoptim)

# A function to get deviance from GCM
# This function is the cost function that the optimizer (nmkb) attempts to minimize. For Maximum Likelihood Estimation, this function calculates the total Deviance, which is equivalent to minimizing the Negative Log-Likelihood (NLL)
GCMutil <- function(theta, stim, exemplars, data, N, retpreds){
  
  # theta = The parameter vector being optimized (the θ the algorithm is searching for).
  # data = The observed number of Category A responses for each stimulus.
  # N = The total number of trials per stimulus.
  # retpreds = A flag (boolean) to decide if the function should return the total deviance or the predictions.
  
  nDat <- length(data)
  dev <- rep(NA, nDat)
  preds <- dev
  
  c <- theta[1]
  w <- theta[2]
  w[2] <- (1-w[1])*theta[3]
  w[3] <- (1-sum(w[1:2]))*theta[4]
  w[4] <- (1-sum(w[1:3]))
  sigma <- theta[5]
  b <- theta[6]
  
  for (i in 1:nDat){
    p <- GCMprednoisy(stim[i,], exemplars, c, w, sigma, b)
    dev[i] <- -2*log(dbinom(data[i] ,size = N,prob = p[1]))
    preds[i] <- p[1]
  }
  
# This loop iterates through every stimulus/data point:
# It calls GCMprednoisy to get the model's predicted probability p[1] of an 'A' response for the i-th stimulus.
# It calculates the Likelihood of observing the actual data point (data[i]) given that prediction, using dbinom(..., prob = p[1]).
# It calculates the Deviance for that data point: Deviance=−2⋅log(Likelihood). Minimizing the sum of deviance is equivalent to minimizing the Negative Log-Likelihood (NLL).
  
  if (retpreds){
    return(preds)
  } else {
    return(sum(dev))
  }
}

# The function returns either:
# 1. the Total Deviance (∑dev) for optimization, or
# 2. the vector of Predictions (preds) for plotting (generating prediction using the best-fitting parameters from optimization)

N <- 2*40 # there were 2 responses per face from 40 ppl. Each of the 40 participants viewed and categorized every single face stimulus two times.

stim <- as.matrix(read.table("faceStim.csv", sep=","))

exemplars <- list(a=stim[1:5,], b= stim[6:10,])

data <- scan(file="facesDataLearners.txt") # The proportions of 'A' responses by the participants for each stimuli (total 34 stimuli)
data <- ceiling(data*N)

bestfit <- 10000

for (w1 in c(0.25,0.5,0.75)){
  for (w2 in c(0.25,0.5,0.75)){
    for (w3 in c(0.25,0.5,0.75)){
      
      # initial_par <- c(1, w1, w2, w3, 1, 0.2)
      # cat("\n--- Starting Optimization ---\n")
      print(c(w1, w2, w3))
      # cat("Initial par vector: ", initial_par, "\n")
      
      fitres <- nmkb(par=c(1,w1,w2,w3,1,0.2),
           fn = function(theta) GCMutil(theta,stim,exemplars,data, N, FALSE),
           lower=c(0,0,0,0,0,-5),
           upper=c(10,1,1,1,10,5),
           control=list(trace=0))
      print(fitres)
      # cat("Optimization COMPLETE.\n")
      # cat("Convergence status: ", fitres$convergence, 
          #" (0 = Success)\n")
      # cat("Final Deviance (Value): ", fitres$value, "\n")
      # cat("Final Parameters (c, w1, w2, w3, sigma, b): ", fitres$par, "\n")
      # cat("-----------------------------------\n")
      
      if (fitres$value<bestfit){
        bestres <- fitres
        bestfit <- fitres$value
      }
    }
  }
}


preds <- GCMutil(bestres$par,stim,exemplars,data, N, TRUE)

#pdf(file="GCMfits.pdf", width=5, height=5)
plot(preds,data/N,
     xlab="Data", ylab="Predictions")
#dev.off()

print(bestres)
theta <- bestres$par
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))
print(w)
```
