---
title: "LearnComputationalModeling"
author: "Krisma Adiwibawa"
format:
  pdf:
    code-overflow: wrap
    include-in-header: _preamble.tex
editor: visual
---

## Chapter 2: From Words to Models

### 2.2 Building a Simulation

#### 2.2.2 The Random Walk Model

Listing 2.1 A simple random-walk model

```{r listing_2.1}
##################################
### A Simple Random-Walk Model ###
##################################
nreps <- 10000
# Number of random-walks (10000 separate decisions)
nsamples <- 2000
# Each decision can last up to 2000 small time-steps of evidence accumulation

drift <- 0.0 # Noninformative stimulus. This is the "pull" toward one choice or the other. Drift = 0.0 means the stimulus is completely neutral (like a fair coin), in other words, the starting point is exactly in the middle between 'upper' and 'lower' decision criterion.
sdrw <- 0.3  # Noise in the evidence as standard deviation. Higher values make the evidence accumulation jump around wildly; lower values make the path of evidence accumulation smoother
criterion <- 3 # Response criterion (the distance of the 'upper' and 'lower' boundaries from origin). The first one to be hit wins the decision (e.g. hits upper = choice A; hits lower choice B).

# These are variables we are going to fill with values later
latencies <- rep(0, nreps) # vector containing zeros
responses <- rep(0, nreps) # vector containing zeros
evidence <- matrix(0, nreps, nsamples+1) # a matrix containing zeros

# Filling the matrix with (simulated) data
for (i in c(1:nreps)) {
  evidence[i,] <- cumsum(c(0, rnorm(nsamples, drift, sdrw))) # evidence accumulation
  p <- which(abs(evidence[i,])>criterion)[1] # the step in the evidence accumulation at which decision is made (the "column" where the accumulated evidence satisfies the criterion treshold)
  responses[i] <- sign(evidence[i,p]) #return +1 or -1
  latencies[i] <- p
}

View(evidence)
```

Listing 2.2 Plot up to 5 random-walk paths

```{r listing_2.2, fig.width=10, fig.height=6}

######################################
### plot up to 5 random-walk paths ###
######################################

dev.new(width = 5, height = 3.75, noRStudioGD = TRUE)

tbpn <- min(nreps, 5) # to-be-plotted-number
plot(1:max(latencies[1:tbpn])+10, # create x-axis only as far as the longest latency
     type="n", # create an empty plot
     las=1, # make the y-axis labels horizontal
     ylim=c(-criterion-.5, criterion+.5), # set y-axis limit, give extra room (.5) above and below criterion 
     ylab="Evidence", xlab="Decision time")
for (i in c(1:tbpn)) {
       lines(evidence[i, 1:(latencies[i]-1)]) # this is the values in row i, which is the evidence sampling. -1 means plot the line just before the decision. ("Show these values linked with lines")
}
abline(h=c(criterion, -criterion), lty="dashed") # create a dashed horizontal line for criterion (upper & lower decision treshold)
```

Listing 2.3 Plot distribution of response latencies from random-walk

```{r listing_2.3}

################################
### plot histogram latencies ###
################################
dev.new(width = 4, height = 6, noRStudioGD = TRUE)

par(mfrow=c(2,1)) # split the plotting area into 2 rows and 1 column

toprt <- latencies[responses>0] # latencies that corresponds to +1
topprop <- length(toprt)/nreps # proportion of top responses relative to all decisions
hist(toprt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Top responses (", as.numeric(topprop),
     ") m=", as.character(signif(mean(toprt),4)),
     sep=""), las=1) # las=1 makes all axis labels horizontal

botrt <- latencies[responses<0] # latencies that corresponds to -1
botprop <- length(botrt)/nreps
hist(botrt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Bottom responses (", as.numeric(botprop),
                ") m=", as.character(signif(mean(botrt), 4)),
                sep=""), las=1)
```

#### 2.2.4 Trial-to-Trial Variability in the Random-Walk Model

Listing 2.4 A random-walk model with trial-to-trial variability

```{r listing_2.4}

##########################################################################
### random walk model with unequal latencies between responses classes ###
##########################################################################

nreps <- 1000
nsamples <- 2000

drift <- 0.03 
# 0 = noninformative stimulus; >0 = informative (not a fair coin; stimulus not neutral)
sdrw <- 0.3
criterion <- 3 
t2tsd  <- c(0.00,0.025) # trial-to-trial standard deviation (trial-to-trial variability)

# These are variables we are going to fill with values later
latencies <- rep(0,nreps) # vector containing zeros
responses <- rep(0,nreps) # vector containing zeros
evidence <- matrix(0, nreps, nsamples+1) # a matrix containing zeros

for (i in c(1:nreps)) { 
  sp <- rnorm(1,0,t2tsd[1]) # starting point
  # Instead of always starting exactly at 0, each trial starts at a slightly different position based on a normal distribution. However, since t2tsd[1] is currently 0.0, the starting point is still effectively 0 for now.
  dr <- rnorm(1,drift,t2tsd[2]) # drift rate
  # Even though the average drift is 0.03, each specific trial gets its own unique drift rate based on a normal distribution. Some trials will have a "stronger" signal than others.
  evidence[i,] <- cumsum(c(sp,rnorm(nsamples,dr,sdrw))) 
  p <-  which(abs(evidence[i,])>criterion)[1]
  responses[i] <- sign(evidence[i,p])
  latencies[i]  <- p
}
```

Listing 2.5

```{r listing_2.5}

######################################
### plot up to 5 random walk paths ###
######################################

dev.new(width = 7, height = 5, noRStudioGD = TRUE)

par(mfrow=c(1,1))

tbpn <- min(nreps,5) # to-be-plotted-number
plot(1:max(latencies[1:tbpn])+10,type="n",las=1,
     ylim=c(-criterion-.5,criterion+.5),
     ylab="Evidence",xlab="Decision time")
for (i in c(1:tbpn)) {
  lines(evidence[i,1:(latencies[i]-1)])
}
abline(h=c(criterion,-criterion),lty="dashed")
```

Listing 2.6

```{r listing_2.6}

####################################
### plot histograms of latencies ###
####################################

dev.new(width = 4, height = 6, noRStudioGD = TRUE)

par(mfrow=c(2,1))

toprt <- latencies[responses>0]
topprop <- length(toprt)/nreps
hist(toprt,col="gray",
     xlab="Decision time", xlim=c(0,max(latencies)),
     main=paste("Top responses (",as.numeric(topprop),
          ") m=",as.character(signif(mean(toprt),4)),
          sep=""),las=1)

botrt <- latencies[responses<0]
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Bottom responses (",as.numeric(botprop),
          ") m=",as.character(signif(mean(botrt),4)),
          sep=""),las=1)
```

## Chapter 3: Basic Parameter Estimation Techniques

### 3.3 Least-Squares Estimation in a Familiar Context

#### 3.3.2 Estimating Regression Parameters

Listing 3.2 R code to generate synthetic data and compute regression parameters in two ways

```{r listing_3.2}
######################################################################
### Creating functions to calculate models prediction (getregpred) ###
### and to calculate the discrepancy (rmsd), which will be used in ###
### the optimization                                               ###
######################################################################
rm(list=ls())

# plot data and current predictions          
getregpred <- function(parms,data) {
  getregpred <- parms["b0"] + parms["b1"]*data[ ,2]
  # This is basically the simple linear regression formula: Y = β0 + β1X
  # getregpred (get regression prediction) is calculating Y (the prediction) by plugging in the parameter value of b0 and b1 into the model.
  
  current_rmsd <- sqrt(mean((getregpred - data[ ,1])^2))
  
  # wait with drawing a graph until key is pressed
  par(ask=TRUE) # we force R to wait until we hit enter before it shows the next graphic
  plot(data[ ,2], type="n", las=1, ylim=c(-2,2), xlim=c(-2,2), xlab="X", ylab="Y",
       main=paste("Y = b0 + (b1 x X)", "\n", "\n",
                  "b0 = ", round(parms["b0"], 3), "    ",
                  "b1 = ", round(parms["b1"], 3), "    ",
                  "RMSD = ", round(current_rmsd, 3)))
  # type="n" creates a blank plot
  par(ask=FALSE)
  points (data[ ,2], data[ ,1], pch=21, bg="gray")
  # fill the empty plot with points, whose location is: x=second column of our data and y=first column of our data
  lines  (data[ ,2], getregpred, lty="solid")
  # then create a line whose position on x-axis is second column of our data and on y-axis is the prediction of our simple linear regression model using the estimated parameters
  
  return(getregpred) # in the end, the getregpred function gives us the value of our simple linear regression model's prediction
}                                           

# This is the Discrepancy Function we want to minimize via optimization
# This function computes the discrepancy between column 1 in our data and the prediction of our simple linear regression model  (calculated using the parameter estimates for 'parms')
rmsd <-function(parms, data1) {
  iter <<- iter + 1
  preds <- getregpred(parms, data1)
  rmsd <- sqrt(sum((preds-data1[ ,1])^2)/length(preds))
  cat(c("RMSD = ", rmsd, sep=""))
  # This shows the RMSD value in the console window
  return(rmsd)
  # if we don't specify what variable should the function return using the function "return()", the last variable computed will automatically be the value returned by the function. To ensure that this function returns the rmsd variable we use return(rmsd).
}
```

Listing 3.1 R code to generate synthetic data and compute regression parameters in two ways

```{r listing_3.1}
#################################
### Creating a synthetic data ###
#################################
# define parameters to generate data
nDataPts  <- 20
rho       <- .8
intercept <- .0

# generate synthetic data
data <- matrix(0,nDataPts,2) # create a matrix of nDataPts rows and 2 columns filled with 0
data[ ,2] <- rnorm(nDataPts) # in the second column, fill the rows with a number of nDataPts value from a standard normal distribution
data[ ,1] <- rnorm(nDataPts)*sqrt(1.0-rho^2) + data[ ,2]*rho + intercept  # in the first column, fill the rows with a number of nDataPts with values, whose 'relationship' with the second column is rho and with 'intercept' of intercept. The total variance is fixed to 1, just like the second column. This is a "perfect world" scenario of a Simple Linear Regression.
View(data)
```

Regression analysis using optim()

```{r}
#########################################################
### do regression analysis using the function optim() ###
#########################################################

# assign starting values 
startParms <- c(-1., .2) 
names(startParms) <- c("b1", "b0")
# obtain parameter estimates
xout <- optim(startParms, rmsd, data1=data)
```

Conventional regression analysis using lm()

```{r}
###########################################
### do conventional regression analysis ###
########## ( as a sanity check ) ########## 
###########################################
lm(data[,1] ~ data[,2])
```

### 3.5 Variability in Parameter Estimates

#### 3.5.1 Bootstraping

Listing 3.3 R code to fit power model of forgetting data from Carpenter et al. (2008)

```{r listing_3.3}
rm(list=ls()) # to clear the entire workspace (Global Environment) so we start the script with a clean slate

#####################################################################
### fitting a Power Law of Forgetting to actual experimental data ###
#####################################################################

# discrepancy function for power forgetting function 
powdiscrep <- function (parms,rec,ri) {                    
  if (any(parms<0)||any(parms>1)) return(1e6)
  # manually set upper and lower bound for the parameter estimate by returning a very big number when outside the desired range
  pow_pred <- parms["a"] *(parms["b"]*ri + 1)^(-parms["c"])
  return(sqrt( sum((pow_pred-rec)^2)/length(ri) ))
  # returning the discrepancy between data and model prediction (RMSD)
}
# The parameters:
#    a = Initial memory strength (the start)
#    b = A scaling factor for time
#    c = The decay rate (how fast is the forgetting)
#    ri = Retention Interval (time elapsed since learning)

# Carpenter et al. (2008) Experiment 1
rec <- c(.93,.88,.86,.66,.47,.34)
# recall data (the proportion of words being recalled at each interval)
ri  <- c(.0035, 1, 2, 7, 14, 42)
# retention interval data (in days)

##########################################################
### Searching for the best-fitting parameter estimates ###
### via Optimization                                   ###
##########################################################

# initialize starting values
sparms <-c(1,.05,.7)
names(sparms) <- c("a","b","c")

# obtain best-fitting estimates via optimization
pout <- optim(sparms,powdiscrep,rec=rec,ri=ri)
# arguments in the optim() above:
#   par = sparms
#   fn = powdiscrep
#   rec=rec and ri=ri are additional arguments needed by the powdiscrep function. Any arguments we list after the function name in optim are "passed through" to the function every time it runs.


pow_pred <- pout$par["a"]*(pout$par["b"]*c(0:max(ri)) + 1)^(-pout$par["c"])
# takes the best-fitting parameters returned by optim in the structure pout and computes the model best-fitting prediction using those best-fitting parameter estimates of the model by interpolating across all observed retention intervals
```

Plot the model and data

```{r}
##############################################
### plot data and best-fitting predictions ###
##############################################
#x11()
dev.new(width = 5, height = 5, noRStudioGD = TRUE)

par(cex.axis=1.2,cex.lab=1.4)
# The axis numbers are made 20% larger, and the axis labels (X and Y titles) are 40% larger.
par(mar=(c(5, 5, 3, 2) + 0.1),las=1)
# This sets the margins around the plot (Bottom, Left, Top, Right). The extra space on the left (5.1) is often needed so the Y-axis label doesn't get cut off.
plot(ri,rec,
     xlab = "Retention Interval (Days)", 
     ylab = "Proportion Items Retained",  
     ylim=c(0.3,1),
     xlim=c(0,max(ri)+1),
     xaxt="n", # This suppresses the X-axis. We do this because we want to draw a custom, highly detailed X-axis below using the axis() command
     type="n") # we tell not to create anything yet, just prepare the canvas
lines(c(0:max(ri)),pow_pred,lwd=1) # This is our model line. lwd is the size of the line.
points(ri,rec,pch=21, bg="dark grey",cex=2) # circles to represent the data. pch is the type of symbol. cex is the size.
dev <- pow_pred[ri+1]
# pow_pred was calculated for every integer day from 0 to 42 (e.g., pow_pred[1] is Day 0, pow_pred[2] is Day 1). Since R starts counting at 1, we add 1 to the day to get the right position in the list. This is the 6 points in our model line exactly above or below the actual rec data.
for (x in c(1:length(ri))) {
  lines(c(ri[x],ri[x]),c(dev[x],rec[x]),lwd=1)
  } # This draws vertical error bars (residuals) for each data point.
    # the position on x axis doesn't move ---> (ri[x],ri[x])
    # but vertically stretch a line ---> (dev[x],rec[x])
axis(1,at=c(0:(max(ri)+1)))
```

##### Perform bootstrapping

```{r}
######################################
### perform bootstrapping analysis ###
######################################

ns  <- 55
# The sample size (number of "subjects" being simulated in each round)
nbs <- 1000
# The number of bootstrap samples (how many times we simulate a synthetic data of each with ns subjects)
bsparms <- matrix(NA,nbs,length(sparms))
# An empty matrix (1,000 rows by 3 columns) to store the "best-fitting" parameters for every simulated experiment.

bspow_pred <- pout$par["a"] *(pout$par["b"]*ri + 1)^(-pout$par["c"])
# This is a list of the 6 predicted probabilities calculated by our model. The calculation uses the best-fitting parameter estimates. 

for (i in c(1:nbs)) {   
    recsynth     <- vapply(bspow_pred, FUN=function(x) mean(rbinom(ns,1,x)), numeric(1))
    # We get 6 observations of recall data, randomly coming from binomial distribution, the parameter of p(1) of that binomial distribution being the predicted each recall proportion produced by our model (bspowpred)
    # We do this nbs times ---> for (i in c(1:nbs))
    # numeric(1) in vapply is a safety feature. It tells R: "I expect exactly one number back when the function is applied to every x." This makes sure that applying the function(x) to every x in bspowpred returns exactly a vector of numeric values the same length.
    
    bsparms[i,]  <- unlist(optim(pout$par,powdiscrep,rec=recsynth,ri=ri)$par)
    # We apply optim() by minimizing the powdiscrep function to the synthetic data (recsynth)
    # We get the best-fitting parameters from this synthetic data as optim(...)$par
    # sometimes the output of an optimizer can be a list. We unlist the vector optim(...)$par  to make sure it is now a vector and not a list and put it as row i in the bsparms metrix
}
```

```{r}
####################################################
### Plot a histogram of the bootstrapping result ###
####################################################

dev.new(width = 8, height = 5, noRStudioGD = TRUE)

# function to plot a histogram
histoplot<-function(x,l4x) {
    hist(x,xlab=l4x,main="",xlim=c(0,1),cex.lab=1.5,cex.axis=1.5)
    lq <- quantile(x,0.025) #lower quantile
    abline(v=lq,lty="dashed",lwd=2) #create abline at lower quantile
    uq <- quantile(x,0.975) #upper quantile
    abline(v=uq,lty="dashed",lwd=2) #create abline at upper quantile
    return(c(lq,uq))
    #the space between these two dashed lines is our confidence interval
}

# Apply the histoplot function to all three parameters separately all at once
# x11(5,2)
par(mfcol=c(1,3),las=1)
for (i in c(1:dim(bsparms)[2])) {
    print(histoplot(bsparms[,i],names(sparms)[i]))
}
```

## Chapter 4: Maximum Likelihood Parameter Estimation

### 4.3 Defining a Probability Distribution

#### 4.3.1 Probability Functions Specified by the Psychological Model

Listing 4.1

```{r listing_4.1}

rm(list=ls())

# Function of the shifted Wald Probability Density
rswald <- function(t, a, m, Ter) {
  ans <- a/sqrt(2*pi*(t-Ter)^3)*exp(-(a-m*(t-Ter))^2/(2*(t-Ter)))
}
# parameters:
#   t = response time
#   m = the drift
#   a = the position of the response boundary
#   Ter = shift term representing non-decision time

# In this case, the probability density function is itself the model of behavior and no further assumptions (or intermediary) functions are needed to relate the model to the data
```

#### 4.3.2 Probability Functions via Data Models

Listing 4.3 Code for obtaining predicted probabilities of responses from GCM

```{r listing_4.3}
# THIS IS THE CONTENT OF THE "GCMpred.R" FILE
rm(list=ls())

# The function that implements the mathematical core of the GCM, calculating the predicted probability of a specific category response (e.g., Category A).

GCMpred <- function(probe, exemplars, c, w) {
  
# probe = A vector representing the new stimulus to be categorized (e.g., the features of a single face). This is the new object whose category is unknown.
  
# exemplars = The memory of past category members.
  #   A list of matrices. exemplars[[1]] (row 1) holds the memory traces
  #   for Category A; exemplars[[2]] (row 2) holds traces for Category B.
  
# c = A single, positive scalar called the scaling parameter. It controls the steepness of the similarity function (how quickly similarity drops as distance increases).
  
# w = A vector of attention weights (one weight for each stimulus dimension/feature). It controls the relative importance of each feature (dimension) during comparison.
  
# calculate likelihod of N_A `A' responses out of N given parameter c
  
# 'stim' is a single vector representing the stimulus to be categorised
# 'exemplars' is a list of exemplars; the first list item is the 'A' exemplars in memory, and the second list item is the `B` exemplars in memory each list item is a matrix in which the rows correspond to individual exemplars
# 'c' is the scaling parameter, and 'w' is a vector giving weighting for each stimulus dimension (the columns in 'stim' and 'exemplars')
  
# note: for a large number of categories we could use lapply to loop across the categories in the list 'exemplars'
  
  dist <- list()
  for (ex in exemplars) {
    dist[[length(dist)+1]] <- apply(as.array(ex), 1, function(x) sqrt(sum(w*(x-probe)^2)))
  }
  
# This loop calculates the psychological distance between the probe and every single exemplar in memory. This uses the weighted Euclidean distance formula. This loop calculates the distances of every exemplar (5 exemplar for category A; 5 exemplar for category B) with each probe.

# The '1' in apply() means "loop by row".
  
# w is multiplied by the squared difference between the probe and the
# exemplar, ensuring that highly attended dimensions (w is large)
# contribute more to the distance.

# The result is a list (dist) where dist[[1]] is a vector of distances to
# all A exemplars, and dist[[2]] is a vector of distances to all B exemplars.
  
  sumsim <- lapply(dist, function(a) sum(exp(-c*a)))
  
# The 'lapply' loop converts the distance (a dissimilarity measure) into psychological similarity using a negative exponential function.

# a is the vector of distances for one category.

# sum(exp(-c*a)) calculates the summed similarity (or category activation) for that category. The parameter c (scaling parameter) modulates how quickly similarity falls off with distance.

# The result is a list (sumsim) where sumsim[[1]] is the total similarity/activation for Category A, and sumsim[[2]] is the total for Category B.
  
  r_prob <- unlist(sumsim)/sum(unlist(sumsim))
  
# The final step converts the category activations into response probabilities using the ratio rule (specifically, Luce's choice rule)
#   Mathematically: P(A) = Activation_A / (Activation_A + Activation_B)
#                        = Summed(Similarity Probe,Exemplar_A) /
#                          (Summed(Similarity Probe&Exemplar_A) +
#                          Summed(Similarity Probe&Exemplar_B))
}
```

Listing 4.2 Linking GCM and the binomial function

```{r listing_4.2}

# This code implements the core calculations of the General Context Model (GCM), a highly influential model in cognitive psychology used to predict how people categorize a new stimulus based on their memory of past examples (exemplars).

source("GCMpred.R")
# The function that implements the mathematical core of the GCM (stored in GCMpred.R file), calculating the predicted probability of a specific category response (e.g., Category A).

N <- 2*80 # there were 2 responses nper face from 80 people
N_A <- round(N*.968) #N_B is implicitly N - N_A. This is the actual experimental data the model needs to explain (In the data collected there are 155 "A"-responses out of 160).

c <- 4 # best-fitting parameter from Nosofsky (1991) 
w <- c(0.19, 0.12, 0.25, 0.45) # best-fitting parameter from Nosofsky (1991)

stim <- as.matrix(read.table("faceStim.csv", sep=",")) # The stimuli used

exemplars <- list(a=stim[1:5,], b=stim[6:10,]) # The exemplar is taken from the first 5 rows of the faceStim.csv (Exemplar for category A) and the next 5 rows (Exemplar for category B)

preds <- GCMpred(stim[1,], exemplars, c, w) # This runs the GCM. It takes only the first stimulus (stim[1,]) and asks: Based on the memory set and the chosen parameters, what is the probability of categorizing this specific probe as A or B?

likelihood <- dbinom(N_A, size=N, prob=preds[1])
# This is the crucial step linking the model prediction to the data using the Binomial Likelihood Function; dbinom() calculates the probability of observing exactly N(A) 'A' responses out of N total responses, given the model's predicted probability (preds[1]).

# The term likelihood is used because we want to calculate what is the likelihood of the parameter values we used above, GIVEN the N_A we observe in the collected data. In the parameter estimation of maximum likelihood method, we are looking for the parameter values that return the highest likelihood, given N_A (the data we collected)

cat("prediction category A: ", preds[[1]], "\n",
    "prediction category B: ", preds[[2]], "\n",
    "likelihood of the parameter estimate used (given the data): ", likelihood, sep="")
```

### 4.4 Finding the Maximum Likelihood

Listing 4.4 R code to implement a version of GCM with a deterministic response rule (Nosofsky, 1991)

```{r listing_4.4}
# THIS IS THE CONTENT OF THE "GCMprednoisy.R" FILE

# This implements a version of the General Context Model (GCM) for categorization, specifically modified to include perceptual or decisionn noise and a response bias.

GCMprednoisy <- function(probe, exemplars, c, w, sigma, b){
  
  # sigma = The standard deviation (SD) of noise. The stochasticity parameter that controls the amount of Gaussian noise in the decision process.
  
  # b = The response bias parameter.
  #     Represents a preference for one category over the other, independent of
  #     the stimulus evidence.
  
  # calculate likelihod of N_A `A' responses out of N given parameter c
  
  # 'stim' is a single vector representing the stimulus to be categorised
  
  # 'exemplars' is a list of exemplars; the first list item is the 'A' exemplars in memory, and the second list item is the `B` exemplars in memory
  
  #   each list item is a matrix in which the rows correspond to individual exemplars
  
  # 'c' is the scaling parameter, and 'w' is a vector giving weighting for each stimulus dimension (the columns in 'stim' and 'exemplars')
  
  # note: for a large number of categories we could use lapply to loop across the categories in the list 'exemplars'
  
  dist <- list()
  for (ex in exemplars){
    dist[[length(dist)+1]] <- apply(as.array(ex), 1, 
                                    function(x) sqrt(sum(w*(x-probe)^2)))
  }
  
  sumsim <- unlist(lapply(dist, function(a) sum(exp(-c*a))))
  
  # this only works for 2 categories
  # we also simplify Nosofsky model in only applying noise at the end
  
  r_prob <- c(0,0)
  r_prob[1] <- pnorm(sumsim[1]-sumsim[2]-b,sd=sigma)
  # This line calculates the probability of choosing Category A (r_prob[1]) using the cumulative distribution function of the Normal distribution (pnorm).
  
  # The probability of choosing A is the probability that the noisy decision evidence exceeds a threshold.
  
  # −b: A constant bias. If b>0, the net evidence must be higher to choose A (i.e., it biases the response toward B). If b<0, it biases the response toward A.
  r_prob[2] <- 1 - r_prob[1]
  return(r_prob)
}
```

Listing 4.5 R code to fit the modified version of GCM to some data

```{r listing_4.5}

source("GCMprednoisy.R")
library(dfoptim)

# A function to get deviance from GCM
# This function is the cost function that the optimizer (nmkb) attempts to minimize. For Maximum Likelihood Estimation, this function calculates the total Deviance, which is equivalent to minimizing the Negative Log-Likelihood (NLL)
GCMutil <- function(theta, stim, exemplars, data, N, retpreds){
  
  # theta = The parameter vector being optimized (the θ the algorithm is searching for).
  # data = The observed number of Category A responses for each stimulus.
  # N = The total number of trials per stimulus.
  # retpreds = A flag (boolean) to decide if the function should return the total deviance or the predictions.
  
  nDat <- length(data)
  dev <- rep(NA, nDat)
  preds <- rep(NA, nDat)
  
  c <- theta[1]
  w <- theta[2]
  w[2] <- (1-w[1])*theta[3]
  w[3] <- (1-sum(w[1:2]))*theta[4]
  w[4] <- (1-sum(w[1:3]))
  sigma <- theta[5]
  b <- theta[6]
  
  for (i in 1:nDat){
    p <- GCMprednoisy(stim[i,], exemplars, c, w, sigma, b)
    dev[i] <- -2*log(dbinom(data[i] ,size = N,prob = p[1]))
    preds[i] <- p[1]
  }
  
# This loop iterates through every stimulus/data point:
# It calls GCMprednoisy to get the model's predicted probability p[1] of an 'A' response for the i-th stimulus.
# It calculates the Likelihood of observing the actual data point (data[i]) given that prediction, using dbinom(..., prob = p[1]).
# It calculates the Deviance for that data point: Deviance=−2⋅log(Likelihood). Minimizing the sum of deviance is equivalent to minimizing the Negative Log-Likelihood (NLL).
  
  if (retpreds){
    return(preds)
  } else {
    return(sum(dev))
  }
}

# The function returns either:
# 1. the Total Deviance (∑dev) for optimization, or
# 2. the vector of Predictions (preds) for plotting (generating prediction using the best-fitting parameters from optimization)

#############################################
### Parameter estimation via optimization ###
#############################################

N <- 2*40 # there were 2 responses per face from 40 ppl. Each of the 40 participants viewed and categorized every single face stimulus two times.

stim <- as.matrix(read.table("faceStim.csv", sep=","))

exemplars <- list(a=stim[1:5,], b= stim[6:10,])

data <- scan(file="facesDataLearners.txt") # The proportions of 'A' responses by the participants for each stimuli (total 34 stimuli)
data <- ceiling(data*N)

#for (w1 in c(0.25,0.5,0.75)){
  #for (w2 in c(0.25,0.5,0.75)){
    #for (w3 in c(0.25,0.5,0.75)){
      
       #initial_par <- c(1, w1, w2, w3, 1, 0.2)
       
# create more elaborate starting values where all parameters have multiple starting values
set.seed(777)
st_c <- runif(100, min=0.02, max=9.98)
st_w1 <- runif(100, min=0.02, max=0.98)
st_w2 <- runif(100, min=0.02, max=0.98)
st_w3 <- runif(100, min=0.02, max=0.98)
st_sigma <- runif(100, min=0.02, max=9.98)
st_b <- runif(100, min=-4.98, max=4.98)

st_val <- cbind(st_c,st_w1, st_w2, st_w3, st_sigma, st_b)
st_val <- split(st_val, seq(nrow(st_val)))
str(st_val)

bestfit <- Inf
bestres <- NULL

for (f in seq_along(st_val)) {
  
  tryCatch({
    
       start <- st_val[[f]]
       
       cat("\n--- Starting Optimization ", f, " of ", length(st_val)," ---\n", sep="")
      # print(c(w1, w2, w3))
       #cat("Initial par vector: ", initial_par, "\n")
       #cat("Initial par vector: ", start, "\n")
       cat("Initial par vector: ", paste(round(start, 3), collapse = ", "), "\n")
      
      fitres <- nmkb(par=start,
           fn = function(theta) {
             out <- GCMutil(theta,stim,exemplars,data,N,FALSE)
             if (is.na(out) || is.infinite(out)) {
               return(1e10) } # Return a huge penalty value instead of NA or Inf
             return(out)  },
           lower=c(0,0,0,0,0,-5),
           upper=c(10,1,1,1,10,5),
           control=list(trace=0))
      
      #print(fitres)
      
       cat("Optimization COMPLETE.\n")
       cat("Convergence status: ", fitres$convergence, 
          " (0 = Success)\n")
       cat("Final Deviance (Value): ", fitres$value, "\n")
       cat("Final Parameters (theta): ", fitres$par, "\n")
       cat("-----------------------------------\n")
      
      if (fitres$value<bestfit){
        bestres <- fitres
        bestfit <- fitres$value
        cat("NEW BEST FOUND!\n")
      }
  })
    #}
  #}
#}
}

preds <- GCMutil(bestres$par,stim,exemplars,data,N, TRUE)

#pdf(file="GCMfits.pdf", width=5, height=5)
dev.new(width = 5, height = 5, noRStudioGD = TRUE)
plot(preds,data/N,
     xlab="Data", ylab="Predictions")
#dev.off()

#bestres
theta <- bestres$par
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))

cat("Best-Fitting Parameters", "\n",
    "c     = ", round(theta[1],2), "\n",
    "w1    = ", round(w[1],2), "\n",
    "w2    = ", round(w[2],3), "\n",
    "w3    = ", round(w[3],2), "\n",
    "w4    = ", round(w[4],2), "\n",
    "sigma = ", round(theta[5],2), "\n",
    "b     = ", round(theta[6],3), sep="")

theta_bs <- bestres$par
```

#### Try bootstrapping to the estimate of GCMprednoisy

```{r}
#################################################################
### Estimate the parameters on snythetic samples ################
#################################################################

# vector preds above contained the predicted proportion of 'A' choices for each 34 stimulus
nbs <- 500 # simulate nbs synthetic samples

bsparms <- matrix(NA, nbs, length(theta))
# empty matrix with nbs rows and columns for parameter estimates from synthetic samples

for (each_nbs in (1:nbs)) {
  
  tryCatch({
  
     propA_synth <- vapply(preds, FUN=function(x) sum(rbinom(N,1,x)), numeric(1))
     # Why sum the rbinom? Because the GCMutil function only accepts the number of 'A' choices and not the proportion/probability. So instead of mean(), we use sum() to return the number of 'A' choices.
     
     bs_bestres <- bestres
     bs_bestfit <- Inf
       
     bs_fitres <- nmkb(par = theta_bs,
                       fn  = function(theta) GCMutil(theta,stim,exemplars,propA_synth,N,FALSE),
                       lower=c(0,0,0,0,0,-5),
                       upper=c(10,1,1,1,10,5),
                       control=list(trace=0))
     
     cat("Optimization for bootstrap sample", each_nbs,"out of", nbs, "completed.\n")
     
             if (!is.na(bs_fitres$value) && bs_fitres$value < bs_bestfit){
                      bs_bestres <- bs_fitres
                      bs_bestfit <- bs_fitres$value
                      }
     
     bs_theta <- bs_bestres$par
     c <- bs_theta[1]
     w <- numeric(4)
     w[1] <- bs_theta[2]
     w[2] <- (1-w[1])*bs_theta[3]
     w[3] <- (1-sum(w[1:2]))*bs_theta[4]
     w[4] <- (1-sum(w[1:3]))
     sigma <- bs_theta[5]
     b <- bs_theta[6]
     
     bsparms[each_nbs,] <- c(c, w[1], w[2], w[3], sigma, b)
     
  }, error = function(e) {
    message(paste("Error in iteration", each_nbs, ":", e))
  })
}
```

Plot the bootstrap estimates

```{r}
# Inspect the range of the bootstrap distribution
get_bs_range <- function(x) {
  return(c(Min = min(x, na.rm = TRUE), 
           Max = max(x, na.rm = TRUE)))
}

range_bs_par <- apply(bsparms, 2, get_bs_range)
colnames(range_bs_par) <- c("c","w1","w2","w3","sigma","b")

# Decide the appropriate range to show in the plots
up_low_bs_par <- list(c(0,5),
                      c(0,1),
                      c(0,0.1),
                      c(0,1),
                      c(0,1.5),
                      c(-0.2,0.2))

# Function to plot a histogram
histoplot <- function(x,range) {
    hist(x,main="",xlim=range,cex.lab=1,cex.axis=1)
    lq <- quantile(x,0.025) #lower quantile
    abline(v=lq,lty="dashed",lwd=2) #create abline at lower quantile
    uq <- quantile(x,0.975) #upper quantile
    abline(v=uq,lty="dashed",lwd=2) #create abline at upper quantile
    return(c(lq,uq))
    #the space between these two dashed lines is our confidence interval
}

# Create the histograms
dev.new(width = 9, height = 5.5, noRStudioGD = TRUE)
par(mfrow = c(2, 3), las = 1)

for (i in 1:ncol(bsparms)) {
    var_name <- colnames(range_bs_par)[i]
    ci <- histoplot(bsparms[, i], up_low_bs_par[[i]])
    title(main = paste(var_name))
}
```

## Chapter 5: Combining Information from Multiple Participants

### 5.3 Fitting Aggregate Data

Listing 5.1 Fitting the Weibull to response times from multiple participants

```{r listing_5.1_5.2}
rm(list=ls())
# This demonstrates two distinct methods for estimating the parameters of a Shifted Weibull Distribution—a common model for RT distributions—and compares their results:

#    1. Quantile Averaging (Aggregate Fit): Fitting the model to the average quantiles across all subjects.

#    (in the next code block)
#    2. Maximum Likelihood Estimation (Individual Fit): Fitting the model to each individual subject's raw RT data.

# This section creates synthetic data for a hypothetical experiment to test the fitting procedures.
#dat <- read.csv(file="rt_data.csv") # uncomment this if you read in data
nsubj <- 30 # Number of simulated participants
nobs <- 20 # Number of observations (trials) per participant
q_p <- c(.1,.3,.5,.7,.9) # Quantiles to be calculated (10th, 30th, ..., 90th percentile)

# The true underlying parameters for the Shifted Weibull distribution are generated for each of the 30 participants. This simulates the natural variability of human cognition (a hierarchical data structure).
shift <- rnorm(nsubj,250,50) #Shift (τ): The minimum RT (or non-decision time).
scale <- rnorm(nsubj,200,50) #Scale (λ): Related to the mean/median of the RT distribution.
shape <- rnorm(nsubj,2,0.25) #Shape (k): Controls the skewness of the distribution.

params <- rbind(shift,scale,shape)

print(rowMeans(params)) # Prints the grand mean of the true parameters

# The core data is generated using the rweibull function, which creates random numbers from the Weibull distribution.
# rows are trials, columns are participants
dat <- apply(params, 2, function(x) rweibull(nobs,shape=x[3],scale=x[2])+x[1])
# Loops through each participant's set of parameters (columns of params).
# This creates a synthetic RT data for each of 30 participants completing 20 trials.

# calculate sample quantiles for each particpant
kk <- apply(dat, 2, function(x) quantile(x, probs=q_p))
# kk is a matrix where rows are the quantile percentiles (0.1, 0.3, etc.) and columns are participants.

###########################################################################################
## FITTING VIA QUANTILE AVERAGING #########################################################
###########################################################################################
# This method simplifies the estimation by fitting the model to the average behavior across all participants.

# average the quantiles
vinq <- rowMeans(kk)
# The average of the 30 columns in kk is taken, resulting in a single vector (vinq) of 5 average quantiles.

# fit the shifted Weibull to averaged quantiles
# This function defines the cost to be minimized. It is a Least Squares approach, minimizing the difference between the predicted quantiles and the average empirical quantiles (vinq).
weib_qdev <- function(x,q_emp, q_p){
  if (any(x<=0)){
    return(10000000)
  }
  q_pred <- qweibull(q_p,shape=x[3],scale=x[2])+x[1] #gives the quantiles from a weibull distribution of a certain shape and scale
  dev <- sqrt(mean((q_pred-q_emp)^2)) # This is the least-square, we want to minimize this.
}

res_quantaveraging <- optim(c(225,225,1),
             function(x) weib_qdev(x, vinq, q_p))
# An initial guess/starting values c(225, 225, 1) is provided.
# optim() finds the single set of parameters that minimizes the RMSD between the predicted and averaged observed quantiles.

print(res_quantaveraging)
```

### 5.4 Fitting Individual Participants

Listing 5.2 Fitting the Weibull to response times from individual participants

```{r listing_5.2}
#########################################################################################
## FITTING INDIVIDUAL PARTICIPANTS ######################################################
#########################################################################################
# This method uses Maximum Likelihood Estimation (MLE) to fit the Shifted Weibull to each individual participant's 20 raw RTs.

# This function is the Negative Log-Likelihood (NLL), or, more accurately, the Deviance (−2×LogLikelihood), which is minimized by the optimizer.
weib_deviance <- function(x,rts){
  if (any(x<=0) || any(rts<x[1])){
    return(10000000)
  }
  likel <- dweibull(rts-x[1],shape=x[3],scale=x[2])
  dev <- sum(-2*log(likel))
}
# dweibull(rts - x[1], ...): Calculates the likelihood of each RT, shifted by the parameter x[1].
# The constraint rts < x[1] is crucial: the Shift parameter τ is the non-decision time, so the predicted distribution must not start after the earliest observed RT.

res_fitind <- apply(dat,2,function(a) optim(c(100,225,1), function(x) weib_deviance(x, a)))
# apply(dat, 2, ...): Loops through each column (participant) in the raw data matrix dat.
# A separate optim() call is run for each of the 30 participants, using their 20 RTs (a).


# Extract parameter estimates and put in to a matrix
# This block extracts the final parameter estimates from the list of 30 optimization results (res) and stores them in a matrix (parest).
# It then calculates the mean and standard deviation of the estimated parameters across all 30 participants.
parest <- matrix(
  unlist(lapply(res_fitind, function(x) x$par)),
  ncol=3, byrow=T)

print(colMeans(parest)) # mean parameter estimates
print(apply(parest,2,sd)) # SD of estimates

# note correlations between parameter estimates

# -----
# The primary purpose of the entire script is to compare the mean parameter estimates from the Aggregate Fit (Quantile Averaging) with the Mean Parameter Estimates from the Individual Fits (MLE) to see how well each method recovers the true population means (which are known from the simulation).
```

### 5.5 Fitting Subgroups of data and Individual Differences

#### 5.5.1 Mixture Modeling

Listing 5.3 Fitting a mixture of Gaussians to a simulated bimodal distribution of saccade latencies

```{r listing_5.3}
library("ggplot2")
rm(list=ls())
# generate some data
set.seed(1540614451)

# This is the simulation set up
N <- 1000 # how many data points
pShort <- 0.3 # the probability that a saccade is express saccade

genpars <- list(c(100,10), # these are the parameters of the two gaussian distribution
                c(150,20)) # namely, the mean and SD

# we assume equal sampling probability for the three (two?) distributions
# I think the above comment is a typo because if pShort is not 0.5 then it is not an equal probability sampling
whichD <- sample(c(1,2),N, replace=TRUE, prob=c(pShort, 1-pShort))

dat <- sapply(whichD, function(x) 
  rnorm(1,genpars[[x]][1],genpars[[x]][2]))
# After genpars[[x]] returns a vector, the single brackets [...] are used to select elements from that vector:
# genpars[[x]][1]: Selects the first element of the vector, which is the mean (μ) parameter for the rnorm() function.
# genpars[[x]][2]: Selects the second element of the vector, which is the standard deviation (σ) parameter for the rnorm() function.
# dat is a collection of N saccades, each saccade comes from either the first gaussian distribution or the second gaussian distribution 

# function needed in EM (Expectation Maximization)
# This is analoguous to the built-in R function weighted.mean, and allows us to calculate a sample standard deviation where different data points are given different weights
weighted.sd <- function(x,w,mu=mean(x)){
  wvar <- sum(w*(x-mu)^2)/
    sum(w)
  return(sqrt(wvar))
}
# Why this function is necessary?
# 1. Partial Membership: In the "M-step" (Maximization), we need to update the standard deviation for each Gaussian. But we shouldn't use the whole dataset equally. A data point that has a 90% probability of belonging to Group 1 should have a large influence on Group 1's standard deviation, and very little influence on Group 2's.
# 2. The Formula: The standard formula for variance is sum((x - mu)^2) / length(x). In a weighted version, we replace the count N with the sum of the weights, and we multiply each squared deviation by its specific weight: um(w * (x - mu)^2) / sum(w)
# 3. R's Limitations: While R has a built-in weighted.mean(), it does not have a built-in weighted.sd(). To update the parameters of our two Gaussians correctly during each iteration of the loop, you have to manually calculate this weighted variance.

# guess parameters (as the starting value for iteration)
mu1 <- mean(dat,1)*0.5
mu2 <- mean(dat, 1)*1.5
# The means are simply estimated from the overall sample mean (shifted a little either way so that they are not identical), and the standard deviation of each distribution is simply set to the sample standard deviation. 
sd1 <- sd(dat)
sd2 <- sd(dat)
# We initialize the mixing proportion (the proportion of data thought to belong to the second distribution, namely ppi), to 0.5. We also initialize oldppi, which will be used to keep track of ppi from the previous iteration.
ppi <- 0.5
oldppi <- 0

iter <- 0

# The following while loop iteratively applies the expectation and maximization steps until the change in ppi between the current run through the loop and the previous one (oldppi) is below a treshold value.
while (abs(ppi-oldppi)>.00001){ # we want the iteration to stop only when the absolute difference (between current iteration ppi and previous iteration ppi) is equal to .00001
  
  oldppi <- ppi
  
  # Within the loop, we first apply the expectation step and calculate the probability that each data point in dat comes from the second (vs. first) Gaussian distribution.
  # Note that this calculation takes ppi into account, as we not only want to know the probability of a data point given each distribution, but must also take into account the base probability (in Bayesian language:the prior) of each distribution.
  
  # The Expectation (E) step is essentially a "soft assignment" phase. Because we don't know which data points belong to which distribution, we use our current best guess of the parameters (μ, σ, and ppi) to calculate the probability that each point belongs to Group 2.
  # If we were doing a "Hard" assignment (like K-means clustering), we would simply say, "This point belongs to Group 1." But in GMMs, data points in the middle could belong to either. The E-step calculates a responsibility (resp), which is a value between 0 and 1.
  # --- If resp=0.9, there is a 90% chance the point belongs to Distribution 2.
  # --- If resp=0.5, the point is exactly in the middle and the model is "unsure."
  
  # E step ("Who belongs where?")
  # The loop enters the E-step. The model looks at every single data point and asks:
  # --- "Given my current guess of where the bells are, what is the probability that this specific data point belongs to Bell #2?"
  # It generates a list of probabilities (resp is abbreviation of responsibility). Some points get a 0.99 (definitely Bell 2), some get a 0.01 (definitely Bell 1), and some in the middle get a 0.5.
  # This is applied to the vector dat (N data points) all at once and produces one result vector (of N data points)
  # resp is the posterior probability!
  resp <- ppi*dnorm(dat,mu2,sd2)/
    ((1-ppi)*dnorm(dat,mu1,sd1) + ppi*dnorm(dat,mu2,sd2))
  # (do you notice this is actually bayes theorem?)
  # ppi (Prior): This is our current "base rate" belief about how common the second distribution is (e.g., "I think 50% of all data belongs to Group 2").
  # dnorm(dat, mu2, sd2) (Likelihood): This is the probability of seeing that specific data point if it came from the second distribution.
  # The Denominator (Total Evidence): This is the sum of (Prior × Likelihood) for both groups. It represents the total probability of seeing that data point under the entire model.
  
  # Then, we apply the maximization step and obtain the maximum likelihood estimates of the parameters of the two Gaussians given the data and the membership probabilities in resp. Because our distributions are Gaussian, we can quickly calculate estimates of the parameters of the two Gaussians, we can quickly calculate estimates of mu and sd by taking the mean and standard deviation of the samples. However, the mean and standard deviation calculations must be weighted by the probability that each score is in each distribution; accordingly, we use the built-in weighted.mean function, and the function weighted.sd we defined earlier.
  
  # M step
  # using the posterior probabilities (resp of each data point), we generate new mu1, mu2, sd1, sd2 
  mu1 <- weighted.mean(dat,1-resp)
  mu2 <- weighted.mean(dat,resp)
  
  sd1 <- weighted.sd(dat,1-resp,mu1)
  sd2 <- weighted.sd(dat,resp,mu2)
  
  iter <- iter + 1 # I added this to mark each iteration
  
  # Finally, we recalculate the mixing proportion in ppi by simply taking the average of the probabilities in resp.
  
  ppi <- mean(resp) # This is the new Prior for the next round. You are basically saying: "The average of all my individual posterior certainties is now my new 'base rate' (i.e. new prior) for the whole group."
  print(data.frame(ppi, mu1, mu2, sd1, sd2, iter))
  
}

  # Once the loop has terminated, the variables mu1, mu2, sd1, and sd2 will hold the final estimates of the parameters of the two Gaussian distributions, and ppi and resp hold information about the probability that data (or a particular data point) belonging to each distribution.

  # If ppi is 0.72, the model believes that 72% of our saccade data belongs to the second Gaussian distribution (mu=150, sd=20)), and the remaining 28% belongs to the first distribution (mu=100, sd=10)).

df <- data.frame(rt=dat)

#pdf(file="GMMexample.pdf", width=5, height=4)
ggplot(df, aes(x = rt)) + 
  geom_histogram(aes(y = ..density..),colour = "black", fill = "white", 
                 binwidth = 3) + 
  stat_function(fun = function(k) (1-ppi)*dnorm(k,mu1,sd1)) +
  stat_function(fun = function(k) ppi*dnorm(k,mu2,sd2)) +
  xlab("RT (ms)") + ylab("Density")
#dev.off()


#############################################################################
### The above method is manual. #############################################
### Below is a pre-built R package to do the same thing, called mixtools. ###
#############################################################################

# mixtools for comparison
library(mixtools) # you'll need to install this library
myEM <- normalmixEM( dat, mu = c(1,4),
                     sigma=c(sd(dat),sd(dat)))

# show the result
cat("updated ppi-dist1: ", myEM$lambda[[1]],"\n",
    "updated ppi-dist2: ", myEM$lambda[[2]], "\n",
    "mu1: ",myEM$mu[[1]], " / sd1: ", myEM$sigma[[1]], "\n",
    "mu2: ",myEM$mu[[2]], " / sd2: ", myEM$sigma[[2]],
    sep="")
```

#### 5.5.2 K-Means Clustering

Listing 5.4 K-Means analysis of free recall data

```{r listing_5.4}
rm(list=ls())
# This code is a simulation and analysis of Serial Position Curves (SPCs) in a memory task (like free recall). It simulates how different types of people remember items from a list and then uses unsupervised machine learning to see if it can "discover" those groups without being told who is who.

# Read in the data
# Rows are participants, columns are serial positions
# spcdat <- read.table("freeAccuracy.txt")

# Or generate some example data
nPrim <- 25
nRec <- 50
nBoth <- 25

# # The code simulates 100 participants, divided into three distinct "memory strategies." Each strategy is defined by a different mathematical curve for the 12 items in the list

ll <- 12
serpos <- 1:ll
nTrials <- 10

# Primacy: High accuracy for the first few items, dropping off quickly.
# exp(-expp*(serpos-1))
primDat <- matrix(rep(0,ll*nPrim),nPrim,ll)
for (j in 1:nPrim){
  asym <- 0.3
  expp <- 1
  tdat <- (1-asym)*exp(-expp*(serpos-1)) + asym
  primDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}

# Recency: The reverse of primacy; high accuracy only for the last few items.
# rev(serpos-1) to flip the exponential curve
recDat <- matrix(rep(0,ll*nRec),nRec,ll)
for (j in 1:nRec){
  asym <- 0.3
  expp <- 1
  tdat <- (1-asym)*exp(-expp*rev(serpos-1)) + asym
  recDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}

# A U-shaped curve (high at start and end, low in the middle).
bothDat <- matrix(rep(0,ll*nBoth),nBoth,ll)
for (j in 1:nBoth){
  asym <- 0.5
  expp <- 1
  pc <- 0.5 * exp(-expp*rev(serpos-1)) + 0.5 * exp(-expp*(serpos-1))
  tdat <- (1-asym)*pc + asym
  bothDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}

spcdat <- rbind(primDat,recDat,bothDat)
View(spcdat)
#------------------------------------------
#pdf(file="gap_plot.pdf", width=4, height=4)
par(mfrow=c(1,1))

library(cluster)
gskmn <- clusGap(spcdat, FUN = kmeans, nstart = 20, K.max = 8, B=500)
plot(gskmn, ylim=c(0.15, 0.5))

#dev.off()

#-------------------------------------------
#pdf(file="kmeansSPC.pdf", width=8, height=4)
par(mfrow=c(1,2))
plot(colMeans(spcdat), ylim=c(0,1), type="b",
     xlab="Serial Position", ylab="Proportion Correct", main=NULL)

kmres <- kmeans(spcdat, centers=3, nstart=10)
matplot(t(kmres$centers), type="b", ylim=c(0,1), 
        xlab="Serial Position", ylab="Proportion Correct")
#dev.off()
# kk <- {}
# 
# for (nClust in 2:7){
#   kmres <- kmeans(spcdat, centers=nClust, nstart=10)
#   #kk <- c(kk, kmres$betweenss/kmres$totss)
#   kk <- c(kk,kmres$tot.withinss)
# }

#plot(2:7, kk, type="b", xlab="N Clusters", ylab="")

```

## Chapter 6: Bayesian Parameter Estimation (Basic Concepts)

### 6.2 Analytic Methods for Obtaining Posteriors

#### 6.2.2 The Prior Distribution

Listing 6.1 Plotting some simple Beta distributions

```{r listing_6.1}
rm(list=ls())

#plot some betas
#curve(dbeta(x, 2, 4),ylim=c(0,6),ylab="Probability Density",las=1)
#curve(dbeta(x, 8, 16),add=TRUE,lty="dashed")
#legend("topright",c("Johnnie","Jane"), inset=0.03,lty=c("solid","dashed"))

# Another neat way to create the same plot
# ------------------  write a function for plotting ------------------
# set up the canvas
plot_beta <- function (current_state, compare_to_jane = TRUE) {

a <- current_state[1]
b <- current_state[2]

plot(NULL, xlim=c(0,1), ylim=c(0,6), las=1,
     xlab="Probability of Success",
     ylab="Probability Density",
     main = paste("Johnnie's success =", a, "out of", a+b))

curve(dbeta(x, a, b), add=TRUE, col="royalblue", lwd=2) # Johnnie's Curve
abline(v = a/(a+b), col="royalblue", lty=3) # Johnnie's mean line

# Optional: Compare to Jane
if (compare_to_jane) {
    curve(dbeta(x, 8, 16), add=TRUE, col="firebrick", lwd=2, lty="dashed")
    abline(v = 8/(8+16), col="firebrick", lty=3)
    
    # Add legend
    legend("topright", c("Johnnie", "Jane"), 
           inset=.05, lty=c("solid", "dashed"),
           col=c("royalblue", "firebrick"), lwd=2)
    }
}
# ---------------------------------------------------

# Data
johnnie_before <- c(2,4)
johnnie_after <- c(2+4,4+8)
johnnie_versions <- list(johnnie_before, johnnie_after)

# Execution
par(mfrow = c(1, length(johnnie_versions)))

for (i in seq_along(johnnie_versions)) {
  plot_beta(johnnie_versions[[i]], compare_to_jane=TRUE)
}
```

```{r}
#the remaining lines are not listed in the book but perform some of the computations mentioned there
probdens_Jane <- pbeta(.48,8,16)-pbeta(.18,8,16)
probdens_Johnnie <- pbeta(.48,2,4)-pbeta(.18,2,4)

cat("There is ", round(probdens_Johnnie,2), " probability that Johnnie's true parameter \"batting success rate\" lies between 0.18 and 0.48", "\n",
    "Meaning, there is ",round(probdens_Johnnie,2)*100,"% chance the true parameter is inside the range 0.18 to 0.48 and there is ",(1-round(probdens_Johnnie,2))*100,"% chance the true parameter is outside that range. Which means, not so convincing.", "\n", "\n",
    "There is ", round(probdens_Jane,2), " probability that Jane's true parameter \"batting success rate\" lies between 0.18 and 0.48", "\n",
    "Meaning, there is ",round(probdens_Jane,2)*100,"% chance the true parameter is inside the range 0.18 to 0.48 and there is ",(1-round(probdens_Jane,2))*100,"% chance the true parameter is outside that range. Which means, more convincing.",sep="")
# Calculating the probability mass within a specific interval ([0.18,0.48]) for both Johnnie and Jane. pbeta() calculates the Cumulative Distribution Function (a certain the area under the curve).
# In Bayesian terms, this is asking: "What is the probability that the true parameter θ lies between 0.18 and 0.48?"
# Why .48 and .18? Because the mean of both distribution is 0.33 and here we want to calculate the area between -0.15 and +0.15 from that mean.

# Usually in Bayesian statistical analysis we calculate our 'confidence' by inspecting the area 95% covered in the middle. The Bayesian term for this is the 95% Credible Interval.
# Using the above method, we simply replace the 0.53 with 0.975 and 0.13 with 0.025. or we can use the quantile function:
lower_johnnie <- qbeta(0.025, 2, 4)
upper_johnnie <- qbeta(0.975, 2, 4)

lower_jane <- qbeta(0.025, 8, 16)
upper_jane <- qbeta(0.975, 8, 16)

cat("There is 95% probability that Johnnie's true parameter \"batting success rate\" lies between ", round(lower_johnnie,2), " and ", round(upper_johnnie,2),". The interval spans ", round(upper_johnnie,2) - round(lower_johnnie,2), " (quite wide ---> uncertain)", sep="")
cat("There is 95% probability that Jane's true parameter \"batting success rate\" lies between ", round(lower_jane,2), " and ", round(upper_jane,2),". The interval spans ", round(upper_jane,2) - round(lower_jane,2), " (narrower ---> more certain)", sep="")

# The uncertainty interval of Johnnie's parameter is wider, not so certain.
# Meanwhile, Jane's parameter uncertainty interval is smaller, more certain.
# The above method is simply chopping off 2.5% edges of the distribution. It's okay when the distribution shape is symetrical.

# Another way is to calculate the Highest Density Interval (HDI), which is the absolute narrowest window that captures 95% of the probability. HDI is more reliable when the distribution shape is skewed.

# Calculate using HDInterval package
#install.packages("HDInterval") #install package to calculate HDI
library(HDInterval)

# Johnnie (Alpha=2, Beta=4)
hdi_johnnie <- hdi(qbeta, credMass = 0.95, shape1 = 2, shape2 = 4)
print(hdi_johnnie)

# Jane (Alpha=8, Beta=16)
hdi_jane <- hdi(qbeta, credMass = 0.95, shape1 = 8, shape2 = 16)
print(hdi_jane)

cat("Johnnie's parameter credible interval spans ", round(hdi_johnnie[2]-hdi_johnnie[1],2), "\n")
cat("Jane's parameter credible interval spans ", round(hdi_jane[2]-hdi_jane[1],2), "\n")
```

Demonstrating Bayesian updating

```{r}
####################################################
### Demonstrating the core of Bayesian Updating. ###
####################################################
# This shows how a person's "Prior" belief changes as they observe more and more data.

#x11(7,7)
dev.new(width = 6, height = 6, noRStudioGD = TRUE)
alpha <- beta <- 12
# When the parameter alpha and beta of a beta distribution is set to 12, the shape is highest at 0.5 and symetric. Using a beta distribution of this shape as a prior distribution means we are completely not sure and give uninformative prior.

# See this plot:
curve(dbeta(x, alpha, beta), ylim=c(0,40), ylab="Probability Density", las=1, lwd=3)

t<-c(12,100,1000)
# This vector represents the number of failures (or "Tails") observed in three different scenarios.

i<-0

results_table <- matrix(NA, nrow = 3, ncol = 2)
colnames(results_table) <- c("With Prior", "Ignoring Prior")
rownames(results_table) <- c("Small Data", "Medium Data", "Large Data")

for (h in c(14,113,1130)){
  # This loop iterates through three corresponding numbers of successes (or "Heads").
  # Small sample: 14 Heads / 12 Tails (0.54)
  # Medium sample: 113 Heads / 100 Tails (0.53)
  # Large sample: 1130 Heads / 1000 Tails (0.53)
  i<-i+1
  curve(dbeta(x, alpha+h, beta+t[i]),add=TRUE,lty=log10(t)+1)
  # lty=log10(t)+1 ---> This is a clever way to change the Line Type automatically. As t gets larger (12, 100, 1000), the log10 value increases (roughly 1, 2, 3), ensuring each scenario has a different dash pattern.
  
  results_table[i, 1] <- round(((alpha + h) / (alpha + h + beta + t[i])),3)
  results_table[i, 2] <- round((h / (h + t[i])),3)
}

legend("topright",c("{14, 26}","{113, 213}", "{1130, 2130}"), 
       inset=.05,lty=c(2:4))
abline(v=0.5,col="red")

print(results_table)

pbeta(.5,1130,1000)
pbeta(.5305164,1130,1000)
```

## Beta Distribution Parameters Calculator

```{r}
beta_params <- function(expectation, var) {
  if (var >= expectation * (1 - expectation)) {
    stop("Variance is too high for the given expectation!")
  }
  
  common_term <- (expectation * (1 - expectation) / var) - 1
  alpha <- expectation * common_term
  beta  <- (1 - expectation) * common_term
  
  return(c(alpha = alpha, beta = beta))
}
```

## Chapter 7: Bayesian Parameter Estimation - Monte Carlo Methods

### 7.1 Markov Chain Monte Carlo Methods

#### 7.1.1 The Metropolis-Hastings Algorithm for MCMC

Listing 7.1 Metropolis-Hastings estimate of the mean of a normal distribution

```{r listing_7.1}
rm(list=ls())
#dev.off()
# Suppose we administer an intelligence test to a child who is one of a group of children considered to be particularly gifted. This child scores y = 144 on a test that is known to be normally distributed with sd = 15 for the population at large. What is the likely value of mu for the population of gifted children, assuming a uniform prior distribution for mu?

# We have one piece of data (y=144) and we are trying to guess the "true" average IQ (μ) of the group this child belongs to.

# Since we assume a Uniform Prior (meaning we have no idea what the IQ should be beforehand), the most likely value for the group average is probably the one we just saw (144). But how certain are we? This code uses MCMC (Markov Chain Monte Carlo) to find that out.

# Imagine you find a strange gold coin. You flip it once and it's Heads.
#
#     The Data: 1 Head.
#     The Question: What is the "true" probability (μ) of this coin landing on heads?
#
# The MCMC Approach: Instead of doing a math equation, you have a "robot" (the code) wander around all possible values from 0 to 1. The robot spends more time at values that "fit" the data (like 0.7 or 0.8) and less time at values that don't (like 0.1).
#
# The for loop is the robot's journey. Here is its logic at every step:
#
# The Proposal: The robot is currently at 150. It closes its eyes and takes a random step (using propsd = 20) and lands at, say, 170.
#
# The Evaluation: It asks: "Is 170 a better explanation for a child scoring 144 than 150 is?"
#
# The Decision:
#     If 170 is better: The robot moves there.
#     If 170 is worse: The robot usually stays put, but occasionally moves there anyway just to explore.

# Here, the robot is wandering around IQ scores to see which ones "fit" the score of 144.

# perform MCMC ---------------------------------------------------------------------------
burnin <- 200
# An IQ of 500 is impossible. The robot starts in the middle of nowhere (the "red" line in your trace plot). It takes about 100-200 steps for the robot to finally "find" the real data around 144.
# Burn-in is the act of throwing away those first 1000 panic-steps so they don't mess up our final average.

chain <- rep(0,5000) # this will be filled with markov-chain
obs <- 144 # a single observed score (an IQ of a gifted child)
propsd <- 20 # tuning parameter (the sd of the proposal distribution)

chain[1] <- 500  # starting value (we seed the chain with a plausible value)
# "We create a plausible starting value for the parameter(s) whose posterior distribution – called the 'target distribution' for the purposes of MCMC – we wish to estimate. This becomes our current sample."
for (i in 2:length(chain)) {
    current <- chain[i-1]
    proposal <- current + rnorm(1,0,propsd)
    if (dnorm(obs,proposal,15) > dnorm(obs,current,15)) { 
       chain[i] <- proposal  # accept proposal
       # dnorm gives us the probability density at 'obs' (x-axis) of a distribution with center 'proposal' and width 15. In this case, we know the true distribution is a normal distribution with sd=15. 
       # "If the true group mean were actually proposal, how likely is it that we would see a score of obs?"
       # If the proposal has the higher density (i.e. is closer to the mean of the target distribution), then it is accepted and becomes the next sample in the chain.
    } else {
       chain[i] <- ifelse(runif(1) < dnorm(obs,proposal,15)/dnorm(obs,current,15),  
                          proposal, 
                          current)
       # If the proposal has lower density, it may still be accepted, but only in proportionality to its relative merit: the probability of acceptance is equal to the ratio between the densities of the proposal and the current sample.
       # In other words, if the proposal has lower density, it is still considered (accepted) but not that often. The lower the density of the proposal the less often it is going to be considered.
       # This consideration/acceptance is random. If the proposal happened to be rejected, we use again the 'current' and the process is repeated.
    }
}

# ----------------------------------------------------------------------------------------

hist(chain)
mean(chain)

# The Density Plot
# The Density Plot (The Answer) shows a bell curve. The peak is at 144.
# The width of this curve tells you the likely range. Even though you saw 144, the MCMC shows the "true" group mean could realistically be anywhere between 120 and 170.
plot(density(chain),las=1,xlab=bquote("Sampled values of "*mu),
     yaxt="n",lwd=2,lty="dashed",
     main="",xlim=c(100,200),ylab="",
     ylim=c(0,max(max(density(chain)$y),
                  max(density(chain[-c(1:burnin)])$y),
                  max(dnorm(c(100:200),144,15)))*1.4))
lines(density(chain[-c(1:burnin)]),lwd=2,lty="solid")
lines(c(100:200),dnorm(c(100:200),144,15),col="red",lwd=2)
mtext("   Density",2,1)
legend("topright",inset=.02,c("Normal PDF","All MCMC","Excluding burnin"),
       lty=c("solid","dashed","solid"),col=c("red","black","black"),lwd=2)

# The Trace-Plot
# The Trace Plot (The Journey) shows the robot starting at 500, rushing down to 144, and then "vibrating" around 144. This vibration represents your uncertainty.
plot(chain,type="l",las=1,xlab="Iteration",ylab="Value of accepted sample")
lines(1:burnin,chain[1:burnin],col="red")

# We are using a random walk to discover a probability distribution.
# Instead of just saying "The answer is 144," the MCMC allows us to say: "The answer is 144, but because I only have one child's score and the test has a standard deviation of 15, I am only this confident about that number."
```

Listing 7.2 Computing posterior normal densities when an informative prior is provided

```{r listing 7.2}

```
