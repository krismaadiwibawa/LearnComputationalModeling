---
title: "LearnComputationalModeling"
author: "Krisma Adiwibawa"
format:
  pdf:
    code-overflow: wrap
    include-in-header: _preamble.tex
editor: visual
---

## Chapter 2: From Words to Models

### 2.2.2 The Random Walk Model

Listing 2.1 A simple random-walk model

```{r listing_2.1}

#random-walk model
nreps <- 10000 #number of random-walks (decisions)
nsamples <- 2000 #number of times that evidence is being sampled for each decision

drift <- 0.0 #noninformative stimulus
             #drift rate: amount of evidence available during sampling
sdrw <- 0.3  #noise in the evidence as standard deviation
criterion <- 3 #response criterion (the distance of the 2 boundaries from origin)

latencies <- rep(0, nreps) #vector containing zeros
responses <- rep(0, nreps) #vector containing zeros
evidence <- matrix(0, nreps, nsamples+1) #a matric containing zeros
for (i in c(1:nreps)) {
  evidence[i,] <- cumsum(c(0, rnorm(nsamples, drift, sdrw))) #evidence accumulation
  p <- which(abs(evidence[i,])>criterion)[1] #the step at which decision is made (the column)
  responses[i] <- sign(evidence[i,p]) #return +1 or -1
  latencies[i] <- p
}

#View(evidence)
```

Listing 2.2 Plot up to 5 random-walk paths

```{r listing_2.2, fig.width=10, fig.height=6}

#plot up to 5 random-walk paths
tbpn <- min(nreps, 5) #to-be-plotted-number
plot(1:max(latencies[1:tbpn])+10, # create x-axis as needed (what the longest latency?
     type="n", #create empty plot
     las=1, #make the y-axis labels horizontal
     ylim=c(-criterion-.5, criterion+.5), #set y-axis limit, give extra room above and below criterion
     ylab="Evidence", xlab="Decision time")
for (i in c(1:tbpn)) {
       lines(evidence[i, 1:(latencies[i]-1)]) #this is the values in row i, which is the evidence sampling. -1 means plot the line just before the decision
}
abline(h=c(criterion, -criterion), lty="dashed") #create a dashed line for criterion (upper & lower decision treshold)
```

Listing 2.3 Plot distribution of response latencies from random-walk

```{r listing_2.3, fig.width=10, fig.height=8}

#plot histogram latencies
par(mfrow=c(2,1)) #split the plotting area into 2 rows and 1 column
toprt <- latencies[responses>0] #latencies that corresponds to +1
topprop <- length(toprt)/nreps #proportion of top responses relative to all trials
hist(toprt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Top responses (", as.numeric(topprop),
     ") m=", as.character(signif(mean(toprt),4)),
     sep=""), las=1)
botrt <- latencies[responses<0] #latencies that corresponds to -1
botprop <- length(botrt)/nreps
hist(botrt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Bottom responses (", as.numeric(botprop),
                ") m=", as.character(signif(mean(botrt), 4))
                )
     )
```

Listing 2.4 A random-walk model with trial-to-trial variability

```{r listing_2.4}

#random walk model with unequal latencies between responses classes
nreps <- 1000
nsamples <- 2000

drift <- 0.03  # 0 = noninformative stimulus; >0 = informative
sdrw <- 0.3
criterion <- 3 
t2tsd  <- c(0.0,0.025)

latencies <- rep(0,nreps)
responses <- rep(0,nreps)
evidence <- matrix(0, nreps, nsamples+1) 
for (i in c(1:nreps)) { 
  sp <- rnorm(1,0,t2tsd[1]) 
  dr <- rnorm(1,drift,t2tsd[2]) 
  evidence[i,] <- cumsum(c(sp,rnorm(nsamples,dr,sdrw))) 
  p <-  which(abs(evidence[i,])>criterion)[1]
  responses[i] <- sign(evidence[i,p])
  latencies[i]  <- p
}
```

Listing 2.5

```{r listing_2.5}

#plot up to 5 random walk paths
tbpn <- min(nreps,5)
plot(1:max(latencies[1:tbpn])+10,type="n",las=1,
     ylim=c(-criterion-.5,criterion+.5),
     ylab="Evidence",xlab="Decision time")
for (i in c(1:tbpn)) {
  lines(evidence[i,1:(latencies[i]-1)])   
}
abline(h=c(criterion,-criterion),lty="dashed")  
```

Listing 2.6

```{r listing_2.6}

#plot histograms of latencies
par(mfrow=c(2,1))
toprt <- latencies[responses>0]
topprop <- length(toprt)/nreps
hist(toprt,col="gray",
     xlab="Decision time", xlim=c(0,max(latencies)),
     main=paste("Top responses (",as.numeric(topprop),
          ") m=",as.character(signif(mean(toprt),4)),
          sep=""),las=1)
botrt <- latencies[responses<0]
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Bottom responses (",as.numeric(botprop),
          ") m=",as.character(signif(mean(botrt),4)),
          sep=""),las=1)
```

## Chapter 4: Maximum Likelihood Parameter Estimation

Listing 4.1

```{r listing_4.1}

rswald <- function(t, a, m, Ter) {
  ans <- a/sqrt(2*pi*(t-Ter)^3)*exp(-(a-m*(t-Ter))^2/(2*(t-Ter)))
}

# The shifted Wald probability density function
```

Listing 4.2

```{r listing_4.2}

# This code implements the core calculations of the General Context Model (GCM), a highly influential model in cognitive psychology used to predict how people categorize a new stimulus based on their memory of past examples (exemplars).

source("GCMpred.R")
# The function that implements the mathematical core of the GCM (stored in GCMpred.R file), calculating the predicted probability of a specific category response (e.g., Category A).

N <- 2*80 # there were 2 responses nper face from 80 people
N_A <- round(N*.968) #N_B is implicitly N - N_A. This is the actual experimental data the model needs to explain (In the data collected there are 155 "A"-responses out of 160).

c <- 4 # best-fitting parameter from Nosofsky (1991) 
w <- c(0.19, 0.12, 0.25, 0.45) # best-fitting parameter from Nosofsky (1991)

stim <- as.matrix(read.table("faceStim.csv", sep=",")) # The stimuli used

exemplars <- list(a=stim[1:5,], b=stim[6:10,]) # The exemplar is taken from the first 5 rows of the faceStim.csv (Exemplar for category A) and the next 5 rows (Exemplar for category B)

preds <- GCMpred(stim[1,], exemplars, c, w) # This runs the GCM. It takes only the first stimulus (stim[1,]) and asks: Based on the memory set and the chosen parameters, what is the probability of categorizing this specific probe as A or B?

likelihood <- dbinom(N_A, size=N, prob=preds[1])
# This is the crucial step linking the model prediction to the data using the Binomial Likelihood Function; dbinom() calculates the probability of observing exactly N(A) 'A' responses out of N total responses, given the model's predicted probability (preds[1]).

# The term likelihood is used because we want to calculate what is the likelihood of the parameter values we used above, GIVEN the N_A we observe in the collected data. In the parameter estimation of maximum likelihood method, we are looking for the parameter values that return the highest likelihood, given N_A (the data we collected)

```

Listing 4.4

```{r listing_4.5}

source("GCMprednoisy.R")
library(dfoptim)

# A function to get deviance from GCM
# This function is the cost function that the optimizer (nmkb) attempts to minimize. For Maximum Likelihood Estimation, this function calculates the total Deviance, which is equivalent to minimizing the Negative Log-Likelihood (NLL)
GCMutil <- function(theta, stim, exemplars, data, N, retpreds){
  
  # theta = The parameter vector being optimized (the θ the algorithm is searching for).
  # data = The observed number of Category A responses for each stimulus.
  # N = The total number of trials per stimulus.
  # retpreds = A flag (boolean) to decide if the function should return the total deviance or the predictions.
  
  nDat <- length(data)
  dev <- rep(NA, nDat)
  preds <- dev
  
  c <- theta[1]
  w <- theta[2]
  w[2] <- (1-w[1])*theta[3]
  w[3] <- (1-sum(w[1:2]))*theta[4]
  w[4] <- (1-sum(w[1:3]))
  sigma <- theta[5]
  b <- theta[6]
  
  for (i in 1:nDat){
    p <- GCMprednoisy(stim[i,], exemplars, c, w, sigma, b)
    dev[i] <- -2*log(dbinom(data[i] ,size = N,prob = p[1]))
    preds[i] <- p[1]
  }
  
# This loop iterates through every stimulus/data point:
# It calls GCMprednoisy to get the model's predicted probability p[1] of an 'A' response for the i-th stimulus.
# It calculates the Likelihood of observing the actual data point (data[i]) given that prediction, using dbinom(..., prob = p[1]).
# It calculates the Deviance for that data point: Deviance=−2⋅log(Likelihood). Minimizing the sum of deviance is equivalent to minimizing the Negative Log-Likelihood (NLL).
  
  if (retpreds){
    return(preds)
  } else {
    return(sum(dev))
  }
}

# The function returns either:
# 1. the Total Deviance (∑dev) for optimization, or
# 2. the vector of Predictions (preds) for plotting (generating prediction using the best-fitting parameters from optimization)

N <- 2*40 # there were 2 responses per face from 40 ppl. Each of the 40 participants viewed and categorized every single face stimulus two times.

stim <- as.matrix(read.table("faceStim.csv", sep=","))

exemplars <- list(a=stim[1:5,], b= stim[6:10,])

data <- scan(file="facesDataLearners.txt") # The proportions of 'A' responses by the participants for each stimuli (total 34 stimuli)
data <- ceiling(data*N)

bestfit <- 10000

for (w1 in c(0.25,0.5,0.75)){
  for (w2 in c(0.25,0.5,0.75)){
    for (w3 in c(0.25,0.5,0.75)){
      
      # initial_par <- c(1, w1, w2, w3, 1, 0.2)
      # cat("\n--- Starting Optimization ---\n")
      print(c(w1, w2, w3))
      # cat("Initial par vector: ", initial_par, "\n")
      
      fitres <- nmkb(par=c(1,w1,w2,w3,1,0.2),
           fn = function(theta) GCMutil(theta,stim,exemplars,data, N, FALSE),
           lower=c(0,0,0,0,0,-5),
           upper=c(10,1,1,1,10,5),
           control=list(trace=0))
      print(fitres)
      # cat("Optimization COMPLETE.\n")
      # cat("Convergence status: ", fitres$convergence, 
          #" (0 = Success)\n")
      # cat("Final Deviance (Value): ", fitres$value, "\n")
      # cat("Final Parameters (c, w1, w2, w3, sigma, b): ", fitres$par, "\n")
      # cat("-----------------------------------\n")
      
      if (fitres$value<bestfit){
        bestres <- fitres
        bestfit <- fitres$value
      }
    }
  }
}


preds <- GCMutil(bestres$par,stim,exemplars,data, N, TRUE)

#pdf(file="GCMfits.pdf", width=5, height=5)
plot(preds,data/N,
     xlab="Data", ylab="Predictions")
#dev.off()

print(bestres)
theta <- bestres$par
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))
print(w)
```

## Chapter 5: Combining Information from Multiple Participants

Listing 5.1

```{r listing_5.1_5.2}

# This R code is a comprehensive simulation and analysis script used in computational modeling, specifically focusing on Reaction Time (RT) data.

# It demonstrates two distinct methods for estimating the parameters of a Shifted Weibull Distribution—a common model for RT distributions—and compares their results:
#
#    1. Quantile Averaging (Aggregate Fit): Fitting the model to the average quantiles across all subjects.
#
#    2. Maximum Likelihood Estimation (Individual Fit): Fitting the model to each individual subject's raw RT data.

# This section creates synthetic data for a hypothetical experiment to test the fitting procedures.
#dat <- read.csv(file="rt_data.csv") # uncomment this if you read in data
nsubj <- 30 # Number of simulated participants
nobs <- 20 # Number of observations (trials) per participant
q_p <- c(.1,.3,.5,.7,.9) # Quantiles to be calculated (10th, 30th, ..., 90th percentile)

# The true underlying parameters for the Shifted Weibull distribution are generated for each of the 30 participants. This simulates the natural variability of human cognition (a hierarchical data structure).
shift <- rnorm(nsubj,250,50) #Shift (τ): The minimum RT (or non-decision time).
scale <- rnorm(nsubj,200,50) #Scale (λ): Related to the mean/median of the RT distribution.
shape <- rnorm(nsubj,2,0.25) #Shape (k): Controls the skewness of the distribution.

params <- rbind(shift,scale,shape)

print(rowMeans(params)) # Prints the grand mean of the true parameters

# The core data is generated using the rweibull function, which creates random numbers from the Weibull distribution.
# rows are trials, columns are participants
dat <- apply(params, 2, function(x) rweibull(nobs,shape=x[3],scale=x[2])+x[1])
# Loops through each participant's set of parameters (columns of params).
# This creates a synthetic RT data for each of 30 participants completing 20 trials.

# calculate sample quantiles for each particpant
kk <- apply(dat, 2, function(x) quantile(x, probs=q_p))
# kk is a matrix where rows are the quantile percentiles (0.1, 0.3, etc.) and columns are participants.

###########################################################################################
## FITTING VIA QUANTILE AVERAGING #########################################################
###########################################################################################
# This method simplifies the estimation by fitting the model to the average behavior across all participants.

# average the quantiles
vinq <- rowMeans(kk)
# The average of the 30 columns in kk is taken, resulting in a single vector (vinq) of 5 average quantiles.

# fit the shifted Weibull to averaged quantiles
# This function defines the cost to be minimized. It is a Least Squares approach, minimizing the difference between the predicted quantiles and the average empirical quantiles (vinq).
weib_qdev <- function(x,q_emp, q_p){
  if (any(x<=0)){
    return(10000000)
  }
  q_pred <- qweibull(q_p,shape=x[3],scale=x[2])+x[1] #gives the quantiles from a weibull distribution of a certain shape and scale
  dev <- sqrt(mean((q_pred-q_emp)^2)) # This is the least-square, we want to minimize this.
}

res_quantaveraging <- optim(c(225,225,1),
             function(x) weib_qdev(x, vinq, q_p))
# An initial guess/starting values c(225, 225, 1) is provided.
# optim() finds the single set of parameters that minimizes the RMSD between the predicted and averaged observed quantiles.

print(res_quantaveraging$par)


#########################################################################################
## FITTING INDIVIDUAL PARTICIPANTS ######################################################
#########################################################################################
# This method uses Maximum Likelihood Estimation (MLE) to fit the Shifted Weibull to each individual participant's 20 raw RTs.

# This function is the Negative Log-Likelihood (NLL), or, more accurately, the Deviance (−2×LogLikelihood), which is minimized by the optimizer.
weib_deviance <- function(x,rts){
  if (any(x<=0) || any(rts<x[1])){
    return(10000000)
  }
  likel <- dweibull(rts-x[1],shape=x[3],scale=x[2])
  dev <- sum(-2*log(likel))
}
# dweibull(rts - x[1], ...): Calculates the likelihood of each RT, shifted by the parameter x[1].
# The constraint rts < x[1] is crucial: the Shift parameter τ is the non-decision time, so the predicted distribution must not start after the earliest observed RT.

res_fitind <- apply(dat,2,function(a) optim(c(100,225,1), function(x) weib_deviance(x, a)))
# apply(dat, 2, ...): Loops through each column (participant) in the raw data matrix dat.
# A separate optim() call is run for each of the 30 participants, using their 20 RTs (a).


# Extract parameter estimates and put in to a matrix
# This block extracts the final parameter estimates from the list of 30 optimization results (res) and stores them in a matrix (parest).
# It then calculates the mean and standard deviation of the estimated parameters across all 30 participants.
parest <- matrix(
  unlist(lapply(res_fitind, function(x) x$par)),
  ncol=3, byrow=T)

print(colMeans(parest)) # mean parameter estimates
print(apply(parest,2,sd)) # SD of estimates

# note correlations between parameter estimates

# -----
# The primary purpose of the entire script is to compare the mean parameter estimates from the Aggregate Fit (Quantile Averaging) with the Mean Parameter Estimates from the Individual Fits (MLE) to see how well each method recovers the true population means (which are known from the simulation).
```

Listing 5.3

```{r listing_5.3}
library("ggplot2")
# generate some data
set.seed(1540614451)

N <- 1000
pShort <- 0.3

genpars <- list(c(100,10),
                c(150,20))

# we assume equal sampling probability for the three distributions
whichD <- sample(c(1,2),N, replace=TRUE, prob=c(pShort, 1-pShort))

dat <- sapply(whichD, function(x) 
  rnorm(1,genpars[[x]][1],genpars[[x]][2]))
# After genpars[[x]] returns a vector, the single brackets [...] are used to select elements from that vector:
# genpars[[x]][1]: Selects the first element of the vector, which is the mean (μ) parameter for the rnorm() function.
# genpars[[x]][2]: Selects the second element of the vector, which is the standard deviation (σ) parameter for the rnorm() function.

# function needed in EM
weighted.sd <- function(x,w,mu=mean(x)){
  wvar <- sum(w*(x-mu)^2)/
    sum(w)
  return(sqrt(wvar))
}

# guess parameters
mu1 <- mean(dat,1)*0.8
mu2 <- mean(dat, 1)*1.2
sd1 <- sd(dat)
sd2 <- sd(dat)
ppi <- 0.5
oldppi <- 0

while (abs(ppi-oldppi)>.00001){ #* \label{line:MultipleParticipants:gmmloop}  *\#
  
  oldppi <- ppi
  
  # E step
  resp <- ppi*dnorm(dat,mu2,sd2)/
    ((1-ppi)*dnorm(dat,mu1,sd1) + ppi*dnorm(dat,mu2,sd2))
  
  # M step
  mu1 <- weighted.mean(dat,1-resp)
  mu2 <- weighted.mean(dat,resp)
  
  sd1 <- weighted.sd(dat,1-resp,mu1)
  sd2 <- weighted.sd(dat,resp,mu2)
  
  ppi <- mean(resp)
  print(ppi)
  
}

df <- data.frame(rt=dat)

#pdf(file="GMMexample.pdf", width=5, height=4)
ggplot(df, aes(x = rt)) + 
  geom_histogram(aes(y = ..density..),colour = "black", fill = "white", 
                 binwidth = 3) + 
  stat_function(fun = function(k) (1-ppi)*dnorm(k,mu1,sd1)) +
  stat_function(fun = function(k) ppi*dnorm(k,mu2,sd2)) +
  xlab("RT (ms)") + ylab("Density")
#dev.off()

# mixtools for comparison

library(mixtools) # you'll need to install this library
myEM <- normalmixEM( dat, mu = c(1,4),
                     sigma=c(sd(dat),sd(dat)))

```
