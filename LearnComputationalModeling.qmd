---
title: "LearnComputationalModeling"
author: "Krisma Adiwibawa"
format:
  pdf:
    code-overflow: wrap
    include-in-header: _preamble.tex
editor: visual
---

## Chapter 2: From Words to Models

### 2.2.2 The Random Walk Model

Listing 2.1 A simple random-walk model

```{r listing_2.1}

#random-walk model
nreps <- 10000 #number of random-walks (decisions)
nsamples <- 2000 #number of times that evidence is being sampled for each decision

drift <- 0.0 #noninformative stimulus
             #drift rate: amount of evidence available during sampling
sdrw <- 0.3  #noise in the evidence as standard deviation
criterion <- 3 #response criterion (the distance of the 2 boundaries from origin)

latencies <- rep(0, nreps) #vector containing zeros
responses <- rep(0, nreps) #vector containing zeros
evidence <- matrix(0, nreps, nsamples+1) #a matric containing zeros
for (i in c(1:nreps)) {
  evidence[i,] <- cumsum(c(0, rnorm(nsamples, drift, sdrw))) #evidence accumulation
  p <- which(abs(evidence[i,])>criterion)[1] #the step at which decision is made (the column)
  responses[i] <- sign(evidence[i,p]) #return +1 or -1
  latencies[i] <- p
}

#View(evidence)
```

Listing 2.2 Plot up to 5 random-walk paths

```{r listing_2.2, fig.width=10, fig.height=6}

#plot up to 5 random-walk paths
tbpn <- min(nreps, 5) #to-be-plotted-number
plot(1:max(latencies[1:tbpn])+10, # create x-axis as needed (what the longest latency?
     type="n", #create empty plot
     las=1, #make the y-axis labels horizontal
     ylim=c(-criterion-.5, criterion+.5), #set y-axis limit, give extra room above and below criterion
     ylab="Evidence", xlab="Decision time")
for (i in c(1:tbpn)) {
       lines(evidence[i, 1:(latencies[i]-1)]) #this is the values in row i, which is the evidence sampling. -1 means plot the line just before the decision
}
abline(h=c(criterion, -criterion), lty="dashed") #create a dashed line for criterion (upper & lower decision treshold)
```

Listing 2.3 Plot distribution of response latencies from random-walk

```{r listing_2.3, fig.width=10, fig.height=8}

#plot histogram latencies
par(mfrow=c(2,1)) #split the plotting area into 2 rows and 1 column
toprt <- latencies[responses>0] #latencies that corresponds to +1
topprop <- length(toprt)/nreps #proportion of top responses relative to all trials
hist(toprt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Top responses (", as.numeric(topprop),
     ") m=", as.character(signif(mean(toprt),4)),
     sep=""), las=1)
botrt <- latencies[responses<0] #latencies that corresponds to -1
botprop <- length(botrt)/nreps
hist(botrt, col="gray",
     xlab="Decision time", xlim=c(0, max(latencies)),
     main=paste("Bottom responses (", as.numeric(botprop),
                ") m=", as.character(signif(mean(botrt), 4))
                )
     )
```

Listing 2.4 A random-walk model with trial-to-trial variability

```{r listing_2.4}

#random walk model with unequal latencies between responses classes
nreps <- 1000
nsamples <- 2000

drift <- 0.03  # 0 = noninformative stimulus; >0 = informative
sdrw <- 0.3
criterion <- 3 
t2tsd  <- c(0.0,0.025)

latencies <- rep(0,nreps)
responses <- rep(0,nreps)
evidence <- matrix(0, nreps, nsamples+1) 
for (i in c(1:nreps)) { 
  sp <- rnorm(1,0,t2tsd[1]) 
  dr <- rnorm(1,drift,t2tsd[2]) 
  evidence[i,] <- cumsum(c(sp,rnorm(nsamples,dr,sdrw))) 
  p <-  which(abs(evidence[i,])>criterion)[1]
  responses[i] <- sign(evidence[i,p])
  latencies[i]  <- p
}
```

Listing 2.5

```{r listing_2.5}

#plot up to 5 random walk paths
tbpn <- min(nreps,5)
plot(1:max(latencies[1:tbpn])+10,type="n",las=1,
     ylim=c(-criterion-.5,criterion+.5),
     ylab="Evidence",xlab="Decision time")
for (i in c(1:tbpn)) {
  lines(evidence[i,1:(latencies[i]-1)])   
}
abline(h=c(criterion,-criterion),lty="dashed")  
```

Listing 2.6

```{r listing_2.6}

#plot histograms of latencies
par(mfrow=c(2,1))
toprt <- latencies[responses>0]
topprop <- length(toprt)/nreps
hist(toprt,col="gray",
     xlab="Decision time", xlim=c(0,max(latencies)),
     main=paste("Top responses (",as.numeric(topprop),
          ") m=",as.character(signif(mean(toprt),4)),
          sep=""),las=1)
botrt <- latencies[responses<0]
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
     xlab="Decision time",xlim=c(0,max(latencies)),
     main=paste("Bottom responses (",as.numeric(botprop),
          ") m=",as.character(signif(mean(botrt),4)),
          sep=""),las=1)
```

## Chapter 4: Maximum Likelihood Parameter Estimation

Listing 4.1

```{r listing_4.1}

rswald <- function(t, a, m, Ter) {
  ans <- a/sqrt(2*pi*(t-Ter)^3)*exp(-(a-m*(t-Ter))^2/(2*(t-Ter)))
}

# The shifted Wald probability density function
```

Listing 4.2

```{r listing_4.2}

# This code implements the core calculations of the General Context Model (GCM), a highly influential model in cognitive psychology used to predict how people categorize a new stimulus based on their memory of past examples (exemplars).

source("GCMpred.R")
# The function that implements the mathematical core of the GCM (stored in GCMpred.R file), calculating the predicted probability of a specific category response (e.g., Category A).

N <- 2*80 # there were 2 responses nper face from 80 people
N_A <- round(N*.968) #N_B is implicitly N - N_A. This is the actual experimental data the model needs to explain (In the data collected there are 155 "A"-responses out of 160).

c <- 4 # best-fitting parameter from Nosofsky (1991) 
w <- c(0.19, 0.12, 0.25, 0.45) # best-fitting parameter from Nosofsky (1991)

stim <- as.matrix(read.table("faceStim.csv", sep=",")) # The stimuli used

exemplars <- list(a=stim[1:5,], b=stim[6:10,]) # The exemplar is taken from the first 5 rows of the faceStim.csv (Exemplar for category A) and the next 5 rows (Exemplar for category B)

preds <- GCMpred(stim[1,], exemplars, c, w) # This runs the GCM. It takes only the first stimulus (stim[1,]) and asks: Based on the memory set and the chosen parameters, what is the probability of categorizing this specific probe as A or B?

likelihood <- dbinom(N_A, size=N, prob=preds[1])
# This is the crucial step linking the model prediction to the data using the Binomial Likelihood Function; dbinom() calculates the probability of observing exactly N(A) 'A' responses out of N total responses, given the model's predicted probability (preds[1]).

# The term likelihood is used because we want to calculate what is the likelihood of the parameter values we used above, GIVEN the N_A we observe in the collected data. In the parameter estimation of maximum likelihood method, we are looking for the parameter values that return the highest likelihood, given N_A (the data we collected)

cat("prediction category A: ", preds[[1]], "\n",
    "prediction category B: ", preds[[2]], "\n",
    "likelihood of the data: ", likelihood, sep="")
```

Listing 4.4

```{r listing_4.5}

source("GCMprednoisy.R")
library(dfoptim)

# A function to get deviance from GCM
# This function is the cost function that the optimizer (nmkb) attempts to minimize. For Maximum Likelihood Estimation, this function calculates the total Deviance, which is equivalent to minimizing the Negative Log-Likelihood (NLL)
GCMutil <- function(theta, stim, exemplars, data, N, retpreds){
  
  # theta = The parameter vector being optimized (the θ the algorithm is searching for).
  # data = The observed number of Category A responses for each stimulus.
  # N = The total number of trials per stimulus.
  # retpreds = A flag (boolean) to decide if the function should return the total deviance or the predictions.
  
  nDat <- length(data)
  dev <- rep(NA, nDat)
  preds <- dev
  
  c <- theta[1]
  w <- theta[2]
  w[2] <- (1-w[1])*theta[3]
  w[3] <- (1-sum(w[1:2]))*theta[4]
  w[4] <- (1-sum(w[1:3]))
  sigma <- theta[5]
  b <- theta[6]
  
  for (i in 1:nDat){
    p <- GCMprednoisy(stim[i,], exemplars, c, w, sigma, b)
    dev[i] <- -2*log(dbinom(data[i] ,size = N,prob = p[1]))
    preds[i] <- p[1]
  }
  
# This loop iterates through every stimulus/data point:
# It calls GCMprednoisy to get the model's predicted probability p[1] of an 'A' response for the i-th stimulus.
# It calculates the Likelihood of observing the actual data point (data[i]) given that prediction, using dbinom(..., prob = p[1]).
# It calculates the Deviance for that data point: Deviance=−2⋅log(Likelihood). Minimizing the sum of deviance is equivalent to minimizing the Negative Log-Likelihood (NLL).
  
  if (retpreds){
    return(preds)
  } else {
    return(sum(dev))
  }
}

# The function returns either:
# 1. the Total Deviance (∑dev) for optimization, or
# 2. the vector of Predictions (preds) for plotting (generating prediction using the best-fitting parameters from optimization)

N <- 2*40 # there were 2 responses per face from 40 ppl. Each of the 40 participants viewed and categorized every single face stimulus two times.

stim <- as.matrix(read.table("faceStim.csv", sep=","))

exemplars <- list(a=stim[1:5,], b= stim[6:10,])

data <- scan(file="facesDataLearners.txt") # The proportions of 'A' responses by the participants for each stimuli (total 34 stimuli)
data <- ceiling(data*N)

bestfit <- 10000

for (w1 in c(0.25,0.5,0.75)){
  for (w2 in c(0.25,0.5,0.75)){
    for (w3 in c(0.25,0.5,0.75)){
      
      # initial_par <- c(1, w1, w2, w3, 1, 0.2)
      # cat("\n--- Starting Optimization ---\n")
      print(c(w1, w2, w3))
      # cat("Initial par vector: ", initial_par, "\n")
      
      fitres <- nmkb(par=c(1,w1,w2,w3,1,0.2),
           fn = function(theta) GCMutil(theta,stim,exemplars,data, N, FALSE),
           lower=c(0,0,0,0,0,-5),
           upper=c(10,1,1,1,10,5),
           control=list(trace=0))
      print(fitres)
      # cat("Optimization COMPLETE.\n")
      # cat("Convergence status: ", fitres$convergence, 
          #" (0 = Success)\n")
      # cat("Final Deviance (Value): ", fitres$value, "\n")
      # cat("Final Parameters (c, w1, w2, w3, sigma, b): ", fitres$par, "\n")
      # cat("-----------------------------------\n")
      
      if (fitres$value<bestfit){
        bestres <- fitres
        bestfit <- fitres$value
      }
    }
  }
}


preds <- GCMutil(bestres$par,stim,exemplars,data, N, TRUE)

#pdf(file="GCMfits.pdf", width=5, height=5)
plot(preds,data/N,
     xlab="Data", ylab="Predictions")
#dev.off()

print(bestres)
theta <- bestres$par
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))
print(w)
```

## Chapter 5: Combining Information from Multiple Participants

Listing 5.1

```{r listing_5.1_5.2}

# This R code is a comprehensive simulation and analysis script used in computational modeling, specifically focusing on Reaction Time (RT) data.

# It demonstrates two distinct methods for estimating the parameters of a Shifted Weibull Distribution—a common model for RT distributions—and compares their results:
#
#    1. Quantile Averaging (Aggregate Fit): Fitting the model to the average quantiles across all subjects.
#
#    2. Maximum Likelihood Estimation (Individual Fit): Fitting the model to each individual subject's raw RT data.

# This section creates synthetic data for a hypothetical experiment to test the fitting procedures.
#dat <- read.csv(file="rt_data.csv") # uncomment this if you read in data
nsubj <- 30 # Number of simulated participants
nobs <- 20 # Number of observations (trials) per participant
q_p <- c(.1,.3,.5,.7,.9) # Quantiles to be calculated (10th, 30th, ..., 90th percentile)

# The true underlying parameters for the Shifted Weibull distribution are generated for each of the 30 participants. This simulates the natural variability of human cognition (a hierarchical data structure).
shift <- rnorm(nsubj,250,50) #Shift (τ): The minimum RT (or non-decision time).
scale <- rnorm(nsubj,200,50) #Scale (λ): Related to the mean/median of the RT distribution.
shape <- rnorm(nsubj,2,0.25) #Shape (k): Controls the skewness of the distribution.

params <- rbind(shift,scale,shape)

print(rowMeans(params)) # Prints the grand mean of the true parameters

# The core data is generated using the rweibull function, which creates random numbers from the Weibull distribution.
# rows are trials, columns are participants
dat <- apply(params, 2, function(x) rweibull(nobs,shape=x[3],scale=x[2])+x[1])
# Loops through each participant's set of parameters (columns of params).
# This creates a synthetic RT data for each of 30 participants completing 20 trials.

# calculate sample quantiles for each particpant
kk <- apply(dat, 2, function(x) quantile(x, probs=q_p))
# kk is a matrix where rows are the quantile percentiles (0.1, 0.3, etc.) and columns are participants.

###########################################################################################
## FITTING VIA QUANTILE AVERAGING #########################################################
###########################################################################################
# This method simplifies the estimation by fitting the model to the average behavior across all participants.

# average the quantiles
vinq <- rowMeans(kk)
# The average of the 30 columns in kk is taken, resulting in a single vector (vinq) of 5 average quantiles.

# fit the shifted Weibull to averaged quantiles
# This function defines the cost to be minimized. It is a Least Squares approach, minimizing the difference between the predicted quantiles and the average empirical quantiles (vinq).
weib_qdev <- function(x,q_emp, q_p){
  if (any(x<=0)){
    return(10000000)
  }
  q_pred <- qweibull(q_p,shape=x[3],scale=x[2])+x[1] #gives the quantiles from a weibull distribution of a certain shape and scale
  dev <- sqrt(mean((q_pred-q_emp)^2)) # This is the least-square, we want to minimize this.
}

res_quantaveraging <- optim(c(225,225,1),
             function(x) weib_qdev(x, vinq, q_p))
# An initial guess/starting values c(225, 225, 1) is provided.
# optim() finds the single set of parameters that minimizes the RMSD between the predicted and averaged observed quantiles.

print(res_quantaveraging$par)


#########################################################################################
## FITTING INDIVIDUAL PARTICIPANTS ######################################################
#########################################################################################
# This method uses Maximum Likelihood Estimation (MLE) to fit the Shifted Weibull to each individual participant's 20 raw RTs.

# This function is the Negative Log-Likelihood (NLL), or, more accurately, the Deviance (−2×LogLikelihood), which is minimized by the optimizer.
weib_deviance <- function(x,rts){
  if (any(x<=0) || any(rts<x[1])){
    return(10000000)
  }
  likel <- dweibull(rts-x[1],shape=x[3],scale=x[2])
  dev <- sum(-2*log(likel))
}
# dweibull(rts - x[1], ...): Calculates the likelihood of each RT, shifted by the parameter x[1].
# The constraint rts < x[1] is crucial: the Shift parameter τ is the non-decision time, so the predicted distribution must not start after the earliest observed RT.

res_fitind <- apply(dat,2,function(a) optim(c(100,225,1), function(x) weib_deviance(x, a)))
# apply(dat, 2, ...): Loops through each column (participant) in the raw data matrix dat.
# A separate optim() call is run for each of the 30 participants, using their 20 RTs (a).


# Extract parameter estimates and put in to a matrix
# This block extracts the final parameter estimates from the list of 30 optimization results (res) and stores them in a matrix (parest).
# It then calculates the mean and standard deviation of the estimated parameters across all 30 participants.
parest <- matrix(
  unlist(lapply(res_fitind, function(x) x$par)),
  ncol=3, byrow=T)

print(colMeans(parest)) # mean parameter estimates
print(apply(parest,2,sd)) # SD of estimates

# note correlations between parameter estimates

# -----
# The primary purpose of the entire script is to compare the mean parameter estimates from the Aggregate Fit (Quantile Averaging) with the Mean Parameter Estimates from the Individual Fits (MLE) to see how well each method recovers the true population means (which are known from the simulation).
```

Listing 5.3

```{r listing_5.3}
library("ggplot2")
# generate some data
set.seed(1540614451)

# This is the simulation set up
N <- 1000 # how many data points
pShort <- 0.3 # the probability that a saccade is express saccade

genpars <- list(c(100,10), # these are the parameters of the two gaussian distribution
                c(150,20)) # namely, the mean and SD

# we assume equal sampling probability for the three (two?) distributions
# I think the above comment is a typo because if pShort is not 0.5 then it is not an equal probability sampling
whichD <- sample(c(1,2),N, replace=TRUE, prob=c(pShort, 1-pShort))

dat <- sapply(whichD, function(x) 
  rnorm(1,genpars[[x]][1],genpars[[x]][2]))
# After genpars[[x]] returns a vector, the single brackets [...] are used to select elements from that vector:
# genpars[[x]][1]: Selects the first element of the vector, which is the mean (μ) parameter for the rnorm() function.
# genpars[[x]][2]: Selects the second element of the vector, which is the standard deviation (σ) parameter for the rnorm() function.
# dat is a collection of N saccades, each saccade comes from either the first gaussian distribution or the second gaussian distribution 

# function needed in EM (Expectation Maximization)
# This is analoguous to the built-in R function weighted.mean, and allows us to calculate a sample standard deviation where different data points are given different weights
weighted.sd <- function(x,w,mu=mean(x)){
  wvar <- sum(w*(x-mu)^2)/
    sum(w)
  return(sqrt(wvar))
}
# Why this function is necessary?
# 1. Partial Membership: In the "M-step" (Maximization), we need to update the standard deviation for each Gaussian. But we shouldn't use the whole dataset equally. A data point that has a 90% probability of belonging to Group 1 should have a large influence on Group 1's standard deviation, and very little influence on Group 2's.
# 2. The Formula: The standard formula for variance is sum((x - mu)^2) / length(x). In a weighted version, we replace the count N with the sum of the weights, and we multiply each squared deviation by its specific weight: um(w * (x - mu)^2) / sum(w)
# 3. R's Limitations: While R has a built-in weighted.mean(), it does not have a built-in weighted.sd(). To update the parameters of your two Gaussians correctly during each iteration of the loop, you have to manually calculate this weighted variance.

# guess parameters (as the starting value for iteration)
mu1 <- mean(dat,1)*0.5
mu2 <- mean(dat, 1)*1.5
# The means are simply estimated from the overall sample mean (shifted a little either way so that they are not identical), and the standard deviation of each distribution is simply set to the sample standard deviation. 
sd1 <- sd(dat)
sd2 <- sd(dat)
# We initialize the mixing proportion (the proportion of data thought to belong to the second distribution, namely ppi), to 0.5. We also initialize oldppi, which will be used to keep track of ppi from the previous iteration.
ppi <- 0.5
oldppi <- 0

iter <- 0

# The following while loop iteratively applies the expectation and maximization steps until the change in ppi between the current run through the loop and the previous one (oldppi) is below a treshold value.
while (abs(ppi-oldppi)>.00001){ # we want the iteration to stop only when the absolute difference (between current iteration ppi and previous iteration ppi) is equal to .00001
  
  oldppi <- ppi
  
  # Within the loop, we first apply the expectation step and calculate the probability that each data point in dat comes from the second (vs. first) Gaussian distribution.
  # Note that this calculation takes ppi into account, as we not only want to know the probability of a data point given each distribution, but must also take into account the base probability (in Bayesian language:the prior) of each distribution.
  
  # The Expectation (E) step is essentially a "soft assignment" phase. Because we don't know which data points belong to which distribution, we use our current best guess of the parameters (μ, σ, and ppi) to calculate the probability that each point belongs to Group 2.
  # If we were doing a "Hard" assignment (like K-means clustering), we would simply say, "This point belongs to Group 1." But in GMMs, data points in the middle could belong to either. The E-step calculates a responsibility (resp), which is a value between 0 and 1.
  # --- If resp=0.9, there is a 90% chance the point belongs to Distribution 2.
  # --- If resp=0.5, the point is exactly in the middle and the model is "unsure."
  
  # E step ("Who belongs where?")
  # The loop enters the E-step. The model looks at every single data point and asks:
  # --- "Given my current guess of where the bells are, what is the probability that this specific data point belongs to Bell #2?"
  # It generates a list of probabilities (resp is abbreviation of responsibility). Some points get a 0.99 (definitely Bell 2), some get a 0.01 (definitely Bell 1), and some in the middle get a 0.5.
  # This is applied to the vector dat (N data points) all at once and produces one result vector (of N data points)
  # resp is the posterior probability!
  resp <- ppi*dnorm(dat,mu2,sd2)/
    ((1-ppi)*dnorm(dat,mu1,sd1) + ppi*dnorm(dat,mu2,sd2))
  # (do you notice this is actually bayes theorem?)
  # ppi (Prior): This is your current "base rate" belief about how common the second distribution is (e.g., "I think 50% of all data belongs to Group 2").
  # dnorm(dat, mu2, sd2) (Likelihood): This is the probability of seeing that specific data point if it came from the second distribution.
  # The Denominator (Total Evidence): This is the sum of (Prior × Likelihood) for both groups. It represents the total probability of seeing that data point under the entire model.
  
  # Then, we apply the maximization step and obtain the maximum likelihood estimates of the parameters of the two Gaussians given the data and the membership probabilities in resp. Because our distributions are Gaussian, we can quickly calculate estimates of the parameters of the two Gaussians, we can quickly calculate estimates of mu and sd by taking the mean and standard deviation of the samples. However, the mean and standard deviation calculations must be weighted by the probability that each score is in each distribution; accordingly, we use the built-in weighted.mean function, and the function weighted.sd we defined earlier.
  
  # M step
  # using the posterior probabilities (resp of each data point), we generate new mu1, mu2, sd1, sd2 
  mu1 <- weighted.mean(dat,1-resp)
  mu2 <- weighted.mean(dat,resp)
  
  sd1 <- weighted.sd(dat,1-resp,mu1)
  sd2 <- weighted.sd(dat,resp,mu2)
  
  iter <- iter + 1 # I added this to mark each iteration
  
  # Finally, we recalculate the mixing proportion in ppi by simply taking the average of the probabilities in resp.
  
  ppi <- mean(resp) # This is the new Prior for the next round. You are basically saying: "The average of all my individual posterior certainties is now my new 'base rate' (i.e. new prior) for the whole group."
  print(data.frame(ppi, mu1, mu2, sd1, sd2, iter))
  
}

  # Once the loop has terminated, the variables mu1, mu2, sd1, and sd2 will hold the final estimates of the parameters of the two Gaussian distributions, and ppi and resp hold information about the probability that data (or a particular data point) belonging to each distribution.

  # If ppi is 0.72, the model believes that 72% of your saccade data belongs to the second Gaussian distribution (mu=150, sd=20)), and the remaining 28% belongs to the first distribution (mu=100, sd=10)).

df <- data.frame(rt=dat)

#pdf(file="GMMexample.pdf", width=5, height=4)
ggplot(df, aes(x = rt)) + 
  geom_histogram(aes(y = ..density..),colour = "black", fill = "white", 
                 binwidth = 3) + 
  stat_function(fun = function(k) (1-ppi)*dnorm(k,mu1,sd1)) +
  stat_function(fun = function(k) ppi*dnorm(k,mu2,sd2)) +
  xlab("RT (ms)") + ylab("Density")
#dev.off()


#############################################################################
### The above method is manual. #############################################
### Below is a pre-built R package to do the same thing, called mixtools. ###
#############################################################################

# mixtools for comparison
library(mixtools) # you'll need to install this library
myEM <- normalmixEM( dat, mu = c(1,4),
                     sigma=c(sd(dat),sd(dat)))

# show the result
cat("updated ppi-dist1: ", myEM$lambda[[1]],"\n",
    "updated ppi-dist2: ", myEM$lambda[[2]], "\n",
    "mu1: ",myEM$mu[[1]], " / sd1: ", myEM$sigma[[1]], "\n",
    "mu2: ",myEM$mu[[2]], " / sd2: ", myEM$sigma[[2]],
    sep="")
```

Listing 5.4

```{r listing_5.4}

# This code is a simulation and analysis of Serial Position Curves (SPCs) in a memory task (like free recall). It simulates how different types of people remember items from a list and then uses unsupervised machine learning to see if it can "discover" those groups without being told who is who.

# Read in the data
# Rows are participants, columns are serial positions
# spcdat <- read.table("freeAccuracy.txt")

# Or generate some example data
nPrim <- 25
nRec <- 50
nBoth <- 25

# # The code simulates 100 participants, divided into three distinct "memory strategies." Each strategy is defined by a different mathematical curve for the 12 items in the list

ll <- 12
serpos <- 1:ll
nTrials <- 10

# Primacy: High accuracy for the first few items, dropping off quickly.
# exp(-expp*(serpos-1))
primDat <- matrix(rep(0,ll*nPrim),nPrim,ll)
for (j in 1:nPrim){
  asym <- 0.3
  expp <- 1
  tdat <- (1-asym)*exp(-expp*(serpos-1)) + asym
  primDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}

# Recency: The reverse of primacy; high accuracy only for the last few items.
# rev(serpos-1) to flip the exponential curve
recDat <- matrix(rep(0,ll*nRec),nRec,ll)
for (j in 1:nRec){
  asym <- 0.3
  expp <- 1
  tdat <- (1-asym)*exp(-expp*rev(serpos-1)) + asym
  recDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}

# A U-shaped curve (high at start and end, low in the middle).
bothDat <- matrix(rep(0,ll*nBoth),nBoth,ll)
for (j in 1:nBoth){
  asym <- 0.5
  expp <- 1
  pc <- 0.5 * exp(-expp*rev(serpos-1)) + 0.5 * exp(-expp*(serpos-1))
  tdat <- (1-asym)*pc + asym
  bothDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}

spcdat <- rbind(primDat,recDat,bothDat)
View(spcdat)
#------------------------------------------
#pdf(file="gap_plot.pdf", width=4, height=4)
par(mfrow=c(1,1))

library(cluster)
gskmn <- clusGap(spcdat, FUN = kmeans, nstart = 20, K.max = 8, B=500)
plot(gskmn, ylim=c(0.15, 0.5))

#dev.off()

#-------------------------------------------
#pdf(file="kmeansSPC.pdf", width=8, height=4)
par(mfrow=c(1,2))
plot(colMeans(spcdat), ylim=c(0,1), type="b",
     xlab="Serial Position", ylab="Proportion Correct", main=NULL)

kmres <- kmeans(spcdat, centers=3, nstart=10)
matplot(t(kmres$centers), type="b", ylim=c(0,1), 
        xlab="Serial Position", ylab="Proportion Correct")
#dev.off()
# kk <- {}
# 
# for (nClust in 2:7){
#   kmres <- kmeans(spcdat, centers=nClust, nstart=10)
#   #kk <- c(kk, kmres$betweenss/kmres$totss)
#   kk <- c(kk,kmres$tot.withinss)
# }

#plot(2:7, kk, type="b", xlab="N Clusters", ylab="")

```
